{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6875f27",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 用于预训练BERT的数据集\n",
    ":label:`sec_bert-dataset`\n",
    "\n",
    "为了预训练 :numref:`sec_bert`中实现的BERT模型，我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科（参见 :numref:`subsec_bert_pretraining_tasks`）的合集上预训练的，但它很难吸引这本书的大多数读者。另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`。\n",
    "\n",
    "与 :numref:`sec_word2vec_data`中用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "342b7589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:38.284931Z",
     "iopub.status.busy": "2023-08-18T07:00:38.284353Z",
     "iopub.status.idle": "2023-08-18T07:00:41.113963Z",
     "shell.execute_reply": "2023-08-18T07:00:41.112838Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a2248",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。我们将更复杂的句子拆分技术的讨论留在本节末尾的练习中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb911790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.118878Z",
     "iopub.status.busy": "2023-08-18T07:00:41.118515Z",
     "iopub.status.idle": "2023-08-18T07:00:41.124582Z",
     "shell.execute_reply": "2023-08-18T07:00:41.123696Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "def load_wikitext_2():\n",
    "    # 从 Hugging Face 下载 wikitext-2 数据集\n",
    "    # 数据集大小：12.1 MB\n",
    "    if platform.system() == \"Darwin\":\n",
    "        cache_dir = \"/Users/usst_ziyi/Programs/AI/d2l-pytorch/data\"\n",
    "    else:\n",
    "        cache_dir = \"D:\\Github\\AI\\d2l-pytorch\\data\"\n",
    "    wikitext2 = load_dataset(\"wikitext\", name=\"wikitext-2-v1\", cache_dir=cache_dir)\n",
    "    # 拆分数据集\n",
    "    train_iter = wikitext2[\"train\"]\n",
    "    valid_iter = wikitext2[\"validation\"]\n",
    "    test_iter = wikitext2[\"test\"]\n",
    "    return train_iter, valid_iter, test_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ddb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki():\n",
    "    train_iter, _, _ = load_wikitext_2()\n",
    "    # 将数据集转换为列表\n",
    "    # lines:List[Str]\n",
    "    lines = [line['text'] for line in train_iter]\n",
    "    # paragraphs:List[List[Str]]\n",
    "    # 只有当你对字符串主动调用会返回列表的方法时，才会产生子列表,比如这里的split()\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "    # paragraphs:List[List[Str]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e18ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the route was first assumed in 1931 as a depression relief project and extended in 1937', 'it remained generally unchanged for the next 60 years before being decommissioned in 1998', 'however , a realignment near lindsay in the late 1950s changed the southern terminus of the route from the centre of the town to southeast of it ; the original route through lindsay became highway <unk> and is now known as kawartha lakes road 17 .']\n",
      "['a bottom @-@ dwelling inhabitant of inshore waters , the diamond stingray favors sandy or muddy <unk> , often near rocky reefs or kelp forests', 'off southern california , it usually occurs from the <unk> zone to a depth of 7 m ( 23 ft ) during the summer , shifting to depths of 13 – 18 m ( 43 – 59 ft ) during late fall and winter', 'for unknown reasons , it prefers to overwinter in kelp forests rather than sandy flats', 'off chile , the diamond stingray occurs at a similar depth of 3 – 30 m ( 10 – 100 ft )', 'on the other hand , this species has been reported from as far down as 355 m ( 1 @,@ 165 ft ) off hawaii , which if accurate would suggest that it utilizes a much greater range of depths than previously realized .']\n",
      "['a protest was held june 14 , 2008 titled \" sea <unk> \" ( a satirical reference to the church of scientology \\'s sea org )', 'protesters dressed up as pirates', 'according to macquarie national news , members of anonymous highlighted the controversial practices of the sea org , including what the protesters believe to be forced contracts where scientologists work below a <unk> wage , that female sea org members who become pregnant are pressured to have <unk> , and that children of families in the organization are made to perform difficult physical labor', 'an international protest held on july 12 , 2008 titled : \" spy vs. sci \" highlighted the church of scientology \\'s office of special affairs', 'a press release by the group posed the question : \" why does something that describes itself as a religion need an intelligence agency that aggressively <unk> critics ? \" the group posted a video in early august 2008 calling for renewed activity in their protest efforts , and planned a subsequent international protest for august 16 , 2008', \"about 35 protesters gathered twice in september 2008 during the first preview and premiere of arthur miller 's play all my sons\", 'they encouraged scientologist katie holmes , wife of tom cruise , to leave the church', 'the most recent international organized protest was held october 18 , 2008', 'members of anonymous dressed as zombies , and highlighted what they described as questionable deaths and suicides of scientologists .']\n",
      "['starting with the delivery of the f @-@ 28s and 737s , all planes were named after norwegian kings', 'the last three f @-@ 27s were also given such names', 'during the 1970s , braathens safe took delivery of eleven 737s , <unk> the four f @-@ 28s', 'the third delivered , ln @-@ <unk> , had a cargo door on the side , making it ideal for cargo flights', 'the three planes delivered in 1979 had extended range tanks making direct flights to the canary islands possible', 'in 1979 , the company started looking into possible replacements for their fleet , considering larger aircraft .']\n",
      "[\"in 1909 , haifa became important to the bahá 'í faith when the remains of the <unk> , founder of the <unk> faith and forerunner of bahá 'u'lláh in the bahá 'í faith , were moved from acre to haifa and interred in the shrine built on mount carmel\", \"bahá <unk> consider the shrine to be their second <unk> place on earth after the shrine of bahá 'u'lláh in acre\", \"its precise location on mount carmel was shown by bahá 'u'lláh himself to his eldest son , `abdu 'l @-@ bahá , in 1891\", \"`abdu 'l @-@ bahá planned the structure , which was designed and completed several years later by his grandson , <unk> <unk>\", \"in a separate room , the remains of `abdu 'l @-@ bahá were buried in november 1921 .\"]\n",
      "['busch was influential in both poetry and illustration , and became a source for future generations of comic artists', \"the katzenjammer kids was inspired by busch 's max and moritz , one of a number of imitations produced in germany and the united states\", 'the wilhelm busch prize and the wilhelm busch museum help maintain his legacy', 'his <unk> anniversary in 2007 was celebrated throughout germany', 'busch remains one of the most influential poets and artists in western europe .']\n",
      "['mole crickets vary in size and appearance , but most of them are of moderate size for an insect , typically between 3 @.@ 2 and 3 @.@ 5 cm ( 1 @.@ 3 and 1 @.@ 4 in ) long as adults', 'they are adapted for underground life and are cylindrical in shape and covered with fine , dense hairs', 'the head , forelimbs , and <unk> are heavily <unk> but the abdomen is rather soft', 'the head bears two threadlike <unk> and a pair of <unk> eyes', 'the two pairs of wings are folded flat over the abdomen ; in most species , the fore wings are short and rounded and the hind wings are <unk> and reach or exceed the tip of the abdomen ; however , in some species the hind wings are reduced in size and the insect is unable to fly', 'the fore legs are flattened for digging but the hind legs are shaped somewhat like the legs of a true cricket ; however , these limbs are more adapted for pushing soil , rather than leaping , which they do rarely and poorly', 'the nymphs resemble the adults apart from the absence of wings and <unk> ; the <unk> become larger after each successive moult .']\n",
      "['bath and north east somerset has <unk> grade i listed buildings , one of the highest concentrations in the country , covered by about 100 english heritage listings', 'the oldest sites within bath are the roman <unk> , for which the foundation piles and an irregular stone chamber lined with lead were built during the roman occupation of britain , although the current building is from the 18th century', 'bath abbey was a norman church built on earlier foundations , although the present building dates from the early 16th century and shows a late <unk> style with flying buttresses and <unk> pinnacles <unk> a <unk> and pierced parapet', 'the medieval era is represented by the remains of the city walls in upper borough <unk> .']\n",
      "['early use of the ns @-@ 10 among engineers include bob clearmountain , rhett davies , and bill <unk> in the us , and nigel <unk> in the uk', 'clearmountain , then a rising star in record production , is often credited for the popularity of the ns @-@ 10 ; phil ward , writing in sound on sound , suggested that clearmountain was probably not the earliest , but was certainly the most influential early <unk>', 'it became a legend that clearmountain had chosen it because it was the worst speaker he could find', 'he was one of a new breed of creative freelance recording engineers and producers who would travel from studio to studio equipped with their own gear that included microphones , and a pair of yamaha ns @-@ 10 , as a reference .']\n",
      "[\"because of the xenon atom 's large , flexible outer electron shell , the nmr spectrum changes in response to surrounding conditions , and can therefore be used as a probe to measure the chemical circumstances around it\", 'for instance xenon dissolved in water , xenon dissolved in hydrophobic solvent , and xenon associated with certain proteins can be distinguished by nmr .']\n"
     ]
    }
   ],
   "source": [
    "paragraphs = _read_wiki()\n",
    "# for 行内所有句子 in 行\n",
    "for paragraph in paragraphs[:10]:\n",
    "    print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5515b",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 为预训练任务定义辅助函数\n",
    "\n",
    "在下文中，我们首先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。\n",
    "\n",
    "### 生成下一句预测任务的数据\n",
    "\n",
    "根据 :numref:`subsec_nsp`的描述，`_get_next_sentence`函数生成二分类任务的训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "246ca273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.128645Z",
     "iopub.status.busy": "2023-08-18T07:00:41.128375Z",
     "iopub.status.idle": "2023-08-18T07:00:41.133471Z",
     "shell.execute_reply": "2023-08-18T07:00:41.132347Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# sentence: 当前句子\n",
    "# next_sentence: 下一个句子\n",
    "# paragraphs: 所有行\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    # 抛硬币决定是否是下一个句子\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 替换掉下一句\n",
    "        # 随机选择一行，然后随机选择一个句子\n",
    "        # 有可能选到当前句子或者当前句子的下一句，这个算噪声\n",
    "        # paragraph:List[List[Str]]\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d432",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "下面的函数通过调用`_get_next_sentence`函数从输入`paragraph`生成用于下一句预测的训练样本。这里`paragraph`是句子列表，其中每个句子都是词元列表。自变量`max_len`指定预训练期间的BERT输入序列的最大长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7686fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.137934Z",
     "iopub.status.busy": "2023-08-18T07:00:41.137439Z",
     "iopub.status.idle": "2023-08-18T07:00:41.143146Z",
     "shell.execute_reply": "2023-08-18T07:00:41.142265Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    # 遍历1行除了最后一个句子外的所有句子\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        # 拼接tokens_a和tokens_b，添加特殊词元\n",
    "        # tokens:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "        # segments:[0, 0, 0, 0, 1, 1, 1, 1]\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        # nsp_data_from_paragraph:List[Tuple[List[str], List[int], bool]]:每个元素是一个元组(tokens, segments, is_next)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86277b80",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "### 生成遮蔽语言模型任务的数据\n",
    ":label:`subsec_prepare_mlm_data`\n",
    "\n",
    "为了从BERT输入序列生成遮蔽语言模型的训练样本，我们定义了以下`_replace_mlm_tokens`函数。在其输入中，`tokens`是表示BERT输入序列的词元的列表，`candidate_pred_positions`是不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及`num_mlm_preds`指示预测的数量（选择15%要预测的随机词元）。在 :numref:`subsec_mlm`中定义遮蔽语言模型任务之后，在每个预测位置，输入可以由特殊的“掩码”词元或随机词元替换，或者保持不变。最后，该函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e3de2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.147428Z",
     "iopub.status.busy": "2023-08-18T07:00:41.146946Z",
     "iopub.status.idle": "2023-08-18T07:00:41.155481Z",
     "shell.execute_reply": "2023-08-18T07:00:41.154569Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# tokens  --mask-->  mlm_input_tokens\n",
    "# 这段代码实现的是 BERT 模型中 Masked Language Model (MLM) 任务的核心逻辑\n",
    "# 对输入序列中的某些词元（tokens）进行遮蔽（masking）或替换，同时记录被遮蔽位置及其原始标签，用于后续的 MLM 训练任务。\n",
    "# tokens:List[str]:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "# candidate_pred_positions: 候选预测位置的索引列表(排除掉'<cls>'和'<sep>'的位置索引)\n",
    "# num_mlm_preds: 要预测的遮蔽词元数量 = tokens长度 * 0.15 四舍五入到最近的整数\n",
    "# vocab: 词汇表对象，用于将索引转换为词元\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 深拷贝副本，为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    mlm_input_tokens = [token for token in tokens] # 深拷贝\n",
    "    # 记录被遮蔽位置及其原始标签\n",
    "    pred_positions_and_labels = [] \n",
    "    # 打乱可遮蔽位置(候选位置)的索引列表\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "    \n",
    "        # 这正是 BERT 论文中描述的 MLM 的 80-10-10 策略\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>' # 80%的时间：将词替换为“<mask>”词元\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position] # 10%的时间：保持词不变\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token) # 10%的时间：用随机词替换该词\n",
    "\n",
    "        # 实施词元替换\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 记录被遮蔽位置及其原始标签(真实值)\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "    # mlm_input_tokens: 更新后的tokens,包含遮蔽语言模型任务输入的词元列表\n",
    "    # pred_positions_and_labels: 被遮蔽位置及其原始标签的元组列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce2383",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "通过调用前述的`_replace_mlm_tokens`函数，以下函数将BERT输入序列（`tokens`）作为输入，并返回输入词元的索引（在 :numref:`subsec_mlm`中描述的可能的词元替换之后）、发生预测的词元索引以及这些预测的标签索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a4650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.160061Z",
     "iopub.status.busy": "2023-08-18T07:00:41.159300Z",
     "iopub.status.idle": "2023-08-18T07:00:41.165820Z",
     "shell.execute_reply": "2023-08-18T07:00:41.164855Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# tokens:List[str]:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "# 这个函数只能处理单句的tokens, 不能处理批量的tokens\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        # 排除'<cls>'和'<sep>'的位置索引\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15)) # round：四舍五入到最近的整数\n",
    "    # tokens  --mask-->  mlm_input_tokens\n",
    "    # pred_positions_and_labels: (position, label)\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    # 对于列表中的每一个元素 x，取它的第 0 个元素（即 x[0]）作为排序的“关键字”。\n",
    "    # 重新回到原始的tokens顺序\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    # 位置索引\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    # 预测标签\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    # vacab[mlm_input_tokens]: 将mlm_input_tokens(添加mask后的tokens)中的每个词元转换为对应的索引\n",
    "    # vacab[mlm_pred_labels]: 将mlm_pred_labels(被遮蔽位置的原始标签)中的每个词元转换为对应的索引\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396550b1",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 将文本转换为预训练数据集\n",
    "\n",
    "现在我们几乎准备好为BERT预训练定制一个`Dataset`类。在此之前，我们仍然需要定义辅助函数`_pad_bert_inputs`来将特殊的“&lt;mask&gt;”词元附加到输入。它的参数`examples`包含来自两个预训练任务的辅助函数`_get_nsp_data_from_paragraph`和`_get_mlm_data_from_tokens`的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6552099b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.170203Z",
     "iopub.status.busy": "2023-08-18T07:00:41.169578Z",
     "iopub.status.idle": "2023-08-18T07:00:41.180126Z",
     "shell.execute_reply": "2023-08-18T07:00:41.179219Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# 整形填充\n",
    "# examples:(mlm_input_ids, pred_positions, mlm_pred_labels_ids, segments, is_next)\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15) # 四舍五入\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels_ids = [], [], []\n",
    "    nsp_labels = []\n",
    "\n",
    "    for (mlm_input_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 填充词元到最大长度:Q\n",
    "        all_token_ids.append(torch.tensor(mlm_input_ids + [vocab['<pad>']] * (max_len - len(mlm_input_ids)), dtype=torch.long))\n",
    "        # 填充句子对的片段索引到最大长度:Q\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数:Q\n",
    "        valid_lens.append(torch.tensor(len(mlm_input_ids), dtype=torch.float32))\n",
    "        # 填充预测位置到最大预测数,因为每个样本的预测位置数量可能不同而且到不了max_num_mlm_preds:C\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉:C\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.float32))\n",
    "        # all_mlm_labels_ids:C\n",
    "        all_mlm_labels_ids.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "        \n",
    "    return (\n",
    "        all_token_ids,           # [B, Q]\n",
    "        all_segments,            # [B, Q]\n",
    "        valid_lens,              # [B]\n",
    "        all_pred_positions,      # [B, C]\n",
    "        all_mlm_weights,         # [B, C]\n",
    "        all_mlm_labels_ids,      # [B, C]\n",
    "        nsp_labels               # [B]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8a88c",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "将用于生成两个预训练任务的训练样本的辅助函数和用于填充输入的辅助函数放在一起，我们定义以下`_WikiTextDataset`类为用于预训练BERT的WikiText-2数据集。通过实现`__getitem__ `函数，我们可以任意访问WikiText-2语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "最初的BERT模型使用词表大小为30000的WordPiece嵌入 :cite:`Wu.Schuster.Chen.ea.2016`。WordPiece的词元化方法是对 :numref:`subsec_Byte_Pair_Encoding`中原有的字节对编码算法稍作修改。为简单起见，我们使用`d2l.tokenize`函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d049c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.184551Z",
     "iopub.status.busy": "2023-08-18T07:00:41.183947Z",
     "iopub.status.idle": "2023-08-18T07:00:41.192539Z",
     "shell.execute_reply": "2023-08-18T07:00:41.191426Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # 0.句子词元化\n",
    "        # paragraphs[行，行中的句子]:List[List[Str]]\n",
    "        # paragraph[句子列表]:List[Str]\n",
    "        # d2l.tokenize:把句子拆分成词元列表:List[List[Str]]\n",
    "        # 左边paragraphs[词元列表]:List[List[List[Str]]]\n",
    "        # paragraphs: List[ List[ List[str] ] ]\n",
    "        #             ↑      ↑      ↑\n",
    "        #            段落   句子   词（token）\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "\n",
    "        # 1.词元词表化(建立索引)\n",
    "        # [表达式 for 外层变量 in 外层可迭代对象 for 内层变量 in 内层可迭代对象]\n",
    "        # paragraphs:List[List[List[Str]]]\n",
    "        # paragraph:List[List[Str]]\n",
    "        # sentence:List[Str]\n",
    "        # sentences:List[List[Str]]:整个数据集的所有句子\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        \n",
    "        examples = []\n",
    "        # 分解\n",
    "        # paragraphs:List[List[List[Str]]]\n",
    "        # paragraph:List[List[Str]]\n",
    "        # 2.获取下一句子预测任务NSP的数据\n",
    "        # _get_nsp_data_from_paragraph:List[Tuple[List[str], List[int], bool]]\n",
    "        # 分解\n",
    "        # example:(tokens, segments, is_next)\n",
    "        # 合并\n",
    "        # examples:List[Tuple[List[str], List[int], bool]]\n",
    "        # 此时examples列表中每个元素是一个元组(tokens, segments, is_next), 这个元组信息来源于之前的一个段落(1行)\n",
    "        examples = [example for paragraph in paragraphs for example in _get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len)]\n",
    "        for example in examples[:3]:\n",
    "            print(example)\n",
    "        \n",
    "        # 3.获取遮蔽语言模型任务MLM的数据\n",
    "        # 这里用到元组拼接，结果：(mlm_input_ids, pred_positions, mlm_pred_labels_ids, segments, is_next)\n",
    "        # 在_get_mlm_data_from_tokens中，所有的词元都被转换为了词元ID\n",
    "        examples = [_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next) for tokens, segments, is_next in examples]\n",
    "        for example in examples[:3]:\n",
    "            print(example)\n",
    "\n",
    "\n",
    "        # 4.填充输入\n",
    "        # _pad_bert_inputs:填充输入，返回元组\n",
    "        # all_token_ids: [B, Q]\n",
    "        # all_segments: [B, Q]\n",
    "        # valid_lens: [B]\n",
    "        # all_pred_positions: [B, C]\n",
    "        # all_mlm_weights: [B, C]\n",
    "        # all_mlm_labels_ids: [B, C]\n",
    "        # nsp_labels: [B]\n",
    "        (\n",
    "            self.all_token_ids,\n",
    "            self.all_segments,\n",
    "            self.valid_lens,\n",
    "            self.all_pred_positions,\n",
    "            self.all_mlm_weights,\n",
    "            self.all_mlm_labels_ids,\n",
    "            self.nsp_labels\n",
    "        ) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx],\n",
    "                self.all_segments[idx],\n",
    "                self.valid_lens[idx], \n",
    "                self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], \n",
    "                self.all_mlm_labels_ids[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede31c0",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "通过使用`_read_wiki`函数和`_WikiTextDataset`类，我们定义了下面的`load_data_wiki`来下载并生成WikiText-2数据集，并从中生成预训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b484a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.197261Z",
     "iopub.status.busy": "2023-08-18T07:00:41.196591Z",
     "iopub.status.idle": "2023-08-18T07:00:41.202074Z",
     "shell.execute_reply": "2023-08-18T07:00:41.201154Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    # paragraphs:List[List[Str]]\n",
    "    paragraphs = _read_wiki()\n",
    "    # train_set: all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    # train_iter: all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b59eb9",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "将批量大小设置为512，将BERT输入序列的最大长度设置为64，我们打印出小批量的BERT预训练样本的形状。注意，在每个BERT输入序列中，为遮蔽语言模型任务预测$10$（$64 \\times 0.15$）个位置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1a8e103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.206083Z",
     "iopub.status.busy": "2023-08-18T07:00:41.205815Z",
     "iopub.status.idle": "2023-08-18T07:00:52.152614Z",
     "shell.execute_reply": "2023-08-18T07:00:52.151321Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<cls>', 'hurricane', 'ingrid', 'was', 'one', 'of', 'two', 'tropical', 'cyclones', ',', 'along', 'with', 'hurricane', 'manuel', ',', 'to', 'strike', 'mexico', 'within', 'a', '24', '‑', 'hour', 'period', ',', 'the', 'first', 'such', 'occurrence', 'since', '1958', '<sep>', 'isbn', '0', '@-@', '345', '@-@', '<unk>', '@-@', 'x', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "(['<cls>', 'ingrid', 'was', 'the', 'ninth', 'named', 'storm', 'and', 'second', 'hurricane', 'of', 'the', '2013', 'atlantic', 'hurricane', 'season', '<sep>', 'the', 'amylostereaceae', 'cause', 'white', 'rot', 'in', 'the', 'wood', 'by', 'disintegrating', 'the', 'tissue', 'component', 'lignin', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "(['<cls>', 'after', 'initially', 'moving', 'westward', 'toward', 'veracruz', ',', 'ingrid', 'turned', 'northeastward', 'away', 'from', 'the', 'coast', '<sep>', '<unk>', 'conditions', 'allowed', 'it', 'to', 'attain', 'hurricane', 'status', 'on', 'september', '14', ',', 'and', 'the', 'next', 'day', 'ingrid', 'attained', 'peak', 'winds', 'of', '140', 'km', '/', 'h', '(', '85', 'mph', ')', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n",
      "([3, 351, 6320, 13, 42, 7, 50, 291, 2, 6, 164, 21, 2, 5874, 6, 10, 2562, 835, 210, 11, 530, 10691, 1291, 319, 6, 5, 40, 94, 8955, 157, 2856, 4, 2, 173, 14, 12054, 14, 2, 14, 2056, 4], [8, 12, 19, 32, 35, 37], [5492, 351, 11, 5695, 19011, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "([3, 6320, 2, 5, 18393, 248, 215, 8, 2, 351, 7, 2, 440, 686, 351, 91, 4, 5, 10033, 1121, 262, 14034, 9, 5, 1933, 22, 0, 5, 4656, 4875, 2, 4], [2, 4, 8, 11, 30], [13, 2701, 105, 5, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "([3, 47, 666, 883, 2, 2, 4244, 6, 6320, 827, 10692, 408, 27, 5, 577, 4, 0, 2, 695, 2, 10, 8956, 351, 1183, 15, 190, 348, 6, 2, 5, 292, 138, 6320, 3742, 1174, 2, 7, 3919, 191, 130, 609, 25, 2973, 766, 24, 4], [4, 5, 17, 19, 28, 35, 41], [3626, 1525, 1399, 26, 8, 696, 25], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n"
     ]
    }
   ],
   "source": [
    "# max_len 表示每个样本的最大长度\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34066341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64])\n",
      "torch.Size([512, 64])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512])\n",
      "tensor([[   3, 5859,   46,  ...,    1,    1,    1],\n",
      "        [   3,    5, 1618,  ...,    1,    1,    1],\n",
      "        [   3,   11,    0,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   3, 1549, 2235,  ...,    1,    1,    1],\n",
      "        [   3,    5,    0,  ...,    1,    1,    1],\n",
      "        [   3,   43,  140,  ...,    1,    1,    1]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "tensor([34., 41., 53., 53., 40., 56., 55., 60., 42., 50., 55., 43., 51., 41.,\n",
      "        43., 34., 58., 57., 53., 44., 46., 48., 43., 42., 47., 56., 61., 61.,\n",
      "        23., 53., 53., 39., 64., 58., 63., 59., 62., 23., 45., 56., 38., 27.,\n",
      "        38., 48., 40., 50., 36., 46., 61., 63., 31., 45., 44., 36., 61., 44.,\n",
      "        54., 46., 38., 34., 27., 62., 61., 33., 56., 64., 50., 54., 33., 48.,\n",
      "        42., 50., 52., 34., 52., 59., 43., 57., 48., 59., 28., 40., 27., 31.,\n",
      "        39., 49., 47., 42., 56., 57., 33., 43., 47., 48., 47., 36., 48., 52.,\n",
      "        46., 54., 43., 58., 43., 54., 30., 49., 29., 52., 52., 48., 50., 41.,\n",
      "        62., 32., 64., 34., 58., 38., 56., 52., 33., 26., 46., 59., 61., 37.,\n",
      "        48., 44., 60., 34., 58., 22., 34., 51., 55., 46., 60., 36., 42., 25.,\n",
      "        44., 53., 44., 55., 40., 31., 53., 50., 42., 39., 51., 61., 59., 48.,\n",
      "        59., 60., 44., 48., 35., 54., 45., 36., 58., 20., 49., 38., 18., 28.,\n",
      "        58., 31., 41., 41., 45., 49., 43., 38., 58., 44., 28., 47., 46., 59.,\n",
      "        53., 49., 27., 61., 50., 41., 46., 47., 57., 62., 36., 41., 50., 40.,\n",
      "        50., 42., 33., 57., 30., 52., 39., 47., 60., 36., 64., 62., 33., 47.,\n",
      "        44., 59., 42., 42., 58., 52., 64., 27., 32., 46., 40., 48., 59., 31.,\n",
      "        64., 36., 55., 45., 46., 37., 54., 53., 38., 62., 39., 62., 58., 62.,\n",
      "        24., 40., 48., 42., 64., 50., 39., 50., 47., 47., 24., 59., 34., 55.,\n",
      "        26., 40., 29., 64., 42., 45., 39., 56., 60., 47., 53., 62., 42., 57.,\n",
      "        35., 58., 46., 49., 54., 63., 57., 55., 59., 36., 51., 56., 52., 47.,\n",
      "        61., 48., 42., 25., 28., 48., 54., 41., 43., 47., 51., 32., 37., 63.,\n",
      "        62., 27., 60., 43., 43., 47., 64., 57., 38., 47., 30., 53., 37., 40.,\n",
      "        51., 52., 33., 48., 40., 37., 30., 57., 55., 53., 38., 49., 54., 45.,\n",
      "        54., 49., 35., 40., 52., 42., 51., 51., 64., 33., 58., 62., 33., 52.,\n",
      "        41., 27., 58., 26., 40., 38., 46., 45., 51., 57., 52., 29., 51., 43.,\n",
      "        59., 54., 62., 56., 55., 49., 46., 55., 43., 30., 54., 39., 51., 44.,\n",
      "        57., 50., 63., 54., 58., 32., 52., 44., 47., 27., 54., 63., 44., 38.,\n",
      "        50., 40., 50., 16., 35., 49., 19., 61., 44., 34., 41., 28., 32., 48.,\n",
      "        55., 48., 47., 49., 54., 55., 63., 16., 60., 42., 53., 35., 57., 51.,\n",
      "        45., 46., 23., 40., 63., 40., 22., 52.,  5., 27., 54., 59., 25., 40.,\n",
      "        62., 53., 25., 46., 37., 59., 42., 38., 60., 39., 19., 60., 28., 49.,\n",
      "        45., 54., 49., 62., 49., 31., 41., 62., 59., 36., 26., 36., 49., 53.,\n",
      "        52., 25., 36., 51., 33., 46., 23., 57., 32., 48., 46., 49., 62., 19.,\n",
      "        48., 43., 53., 26., 61., 63., 61., 50., 44., 50., 11., 41., 33., 63.,\n",
      "        44., 59., 50., 59., 37., 50., 47., 50., 49., 37., 50., 47., 59., 39.,\n",
      "        47., 25., 40., 60., 28., 17., 49., 58., 44., 32., 46., 63., 25., 59.,\n",
      "        46., 52., 53., 61., 54., 57., 57., 31.])\n",
      "tensor([[ 9, 13, 17,  ...,  0,  0,  0],\n",
      "        [ 5,  9, 16,  ...,  0,  0,  0],\n",
      "        [ 4,  9, 19,  ..., 49,  0,  0],\n",
      "        ...,\n",
      "        [ 9, 15, 17,  ..., 43, 54,  0],\n",
      "        [ 7,  8, 10,  ..., 47, 48,  0],\n",
      "        [ 8, 11, 15,  ...,  0,  0,  0]])\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[ 3127,     9,  1400,  ...,     0,     0,     0],\n",
      "        [14593, 19340,  2817,  ...,     0,     0,     0],\n",
      "        [  936,   353,    37,  ...,  8232,     0,     0],\n",
      "        ...,\n",
      "        [  149,  1387,     5,  ...,     5,    24,     0],\n",
      "        [  329,  3680,  1601,  ...,   370,     7,     0],\n",
      "        [    5,     8,     5,  ...,     0,     0,     0]])\n",
      "tensor([1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
      "        1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
      "        1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
      "        1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
      "        0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 1])\n",
      "pos: tensor([ 9, 13, 17, 25, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  9, 16, 25, 34, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 19, 20, 23, 26, 44, 49,  0,  0])\n",
      "pos: tensor([ 8, 12, 17, 27, 30, 38, 43, 46,  0,  0])\n",
      "pos: tensor([ 4,  8, 14, 21, 22, 28,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 11, 30, 31, 36, 37, 52,  0,  0])\n",
      "pos: tensor([ 2,  7, 24, 26, 35, 41, 49, 51,  0,  0])\n",
      "pos: tensor([ 2,  9, 15, 16, 24, 25, 33, 49, 50,  0])\n",
      "pos: tensor([ 4, 10, 16, 18, 19, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 12, 14, 15, 20, 22, 32, 43,  0,  0])\n",
      "pos: tensor([ 2,  3,  7, 13, 20, 27, 33, 50,  0,  0])\n",
      "pos: tensor([ 4, 17, 22, 26, 28, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 10, 19, 20, 28, 34, 39, 44,  0,  0])\n",
      "pos: tensor([ 1,  7, 15, 17, 35, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  6,  7, 19, 33, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 11, 16, 21, 28,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  5,  8, 11, 19, 21, 37, 46, 55,  0])\n",
      "pos: tensor([ 7, 17, 20, 22, 25, 29, 34, 51, 54,  0])\n",
      "pos: tensor([ 9, 12, 15, 17, 32, 35, 38, 48,  0,  0])\n",
      "pos: tensor([ 4,  7, 10, 13, 17, 19, 30,  0,  0,  0])\n",
      "pos: tensor([ 3, 16, 21, 26, 30, 31, 35,  0,  0,  0])\n",
      "pos: tensor([ 9, 20, 23, 25, 29, 31, 36,  0,  0,  0])\n",
      "pos: tensor([ 1, 11, 14, 24, 27, 40,  0,  0,  0,  0])\n",
      "pos: tensor([10, 16, 17, 23, 25, 35,  0,  0,  0,  0])\n",
      "pos: tensor([10, 12, 13, 19, 33, 36, 44,  0,  0,  0])\n",
      "pos: tensor([ 4, 13, 17, 19, 34, 37, 44, 48,  0,  0])\n",
      "pos: tensor([ 2,  6,  9, 27, 28, 39, 40, 48, 55,  0])\n",
      "pos: tensor([ 9, 14, 30, 31, 46, 47, 53, 55, 58,  0])\n",
      "pos: tensor([ 1, 16, 20,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6,  7, 10, 13, 23, 35, 38, 43,  0,  0])\n",
      "pos: tensor([ 2, 22, 24, 31, 35, 41, 43, 51,  0,  0])\n",
      "pos: tensor([ 3, 24, 28, 29, 30, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  4, 18, 23, 33, 41, 44, 47, 57, 62])\n",
      "pos: tensor([ 4,  6,  7,  8, 37, 40, 45, 49, 51,  0])\n",
      "pos: tensor([ 2, 23, 30, 31, 37, 40, 44, 47, 55,  0])\n",
      "pos: tensor([19, 23, 25, 29, 32, 44, 45, 47, 55,  0])\n",
      "pos: tensor([ 4, 10, 11, 12, 20, 23, 26, 48, 55,  0])\n",
      "pos: tensor([ 1, 12, 16,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 11, 18, 21, 40, 41,  0,  0,  0])\n",
      "pos: tensor([ 4,  8, 12, 19, 20, 26, 32, 36,  0,  0])\n",
      "pos: tensor([10, 16, 17, 21, 32, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  8, 10, 18,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  8, 15, 18, 26, 32,  0,  0,  0,  0])\n",
      "pos: tensor([18, 23, 28, 30, 37, 39, 41,  0,  0,  0])\n",
      "pos: tensor([ 2,  8, 11, 27, 28, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  6,  9, 12, 13, 17, 42, 48,  0,  0])\n",
      "pos: tensor([ 3,  4, 22, 28, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  6, 11, 23, 26, 32, 43,  0,  0,  0])\n",
      "pos: tensor([14, 16, 18, 20, 25, 40, 41, 49, 57,  0])\n",
      "pos: tensor([ 8, 18, 19, 32, 44, 47, 54, 60, 61,  0])\n",
      "pos: tensor([ 8, 10, 13, 15, 25,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 10, 16, 21, 35, 40,  0,  0,  0])\n",
      "pos: tensor([ 1,  2,  4, 22, 28, 36, 39,  0,  0,  0])\n",
      "pos: tensor([ 5,  6, 18, 20, 25,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8,  9, 14, 21, 28, 29, 31, 36, 38,  0])\n",
      "pos: tensor([ 4, 13, 19, 24, 30, 33, 41,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 10, 18, 19, 27, 34, 38,  0,  0])\n",
      "pos: tensor([ 1,  5,  6, 17, 22, 34, 36,  0,  0,  0])\n",
      "pos: tensor([13, 17, 20, 21, 22, 32,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  5,  6, 15, 21,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 15, 23, 24,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([14, 17, 20, 34, 43, 48, 50, 57, 58,  0])\n",
      "pos: tensor([ 6,  7, 22, 33, 36, 37, 38, 40, 51,  0])\n",
      "pos: tensor([ 3,  7, 14, 27, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([11, 20, 32, 35, 40, 41, 44, 52,  0,  0])\n",
      "pos: tensor([ 5, 13, 32, 36, 37, 44, 49, 57, 61, 62])\n",
      "pos: tensor([ 2,  8, 20, 21, 29, 32, 33, 38,  0,  0])\n",
      "pos: tensor([ 1, 25, 32, 47, 48, 49, 51, 52,  0,  0])\n",
      "pos: tensor([ 3,  5,  7, 11, 13,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 18, 20, 21, 25, 40, 45,  0,  0,  0])\n",
      "pos: tensor([ 5, 12, 15, 17, 20, 40,  0,  0,  0,  0])\n",
      "pos: tensor([11, 18, 20, 32, 39, 46, 47, 48,  0,  0])\n",
      "pos: tensor([ 2,  8, 24, 30, 33, 34, 37, 47,  0,  0])\n",
      "pos: tensor([ 3, 13, 16, 25, 32,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 12, 16, 18, 21, 28, 32, 37,  0,  0])\n",
      "pos: tensor([ 2,  4,  9, 11, 16, 24, 28, 53, 57,  0])\n",
      "pos: tensor([ 7,  9, 14, 16, 22, 26,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 13, 14, 29, 31, 35, 40, 43,  0])\n",
      "pos: tensor([10, 12, 18, 20, 29, 39, 40,  0,  0,  0])\n",
      "pos: tensor([10, 14, 15, 20, 27, 36, 43, 50, 57,  0])\n",
      "pos: tensor([ 1,  9, 12, 14,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 14, 19, 30, 32, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 18, 25,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 10, 15, 23, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 15, 17, 18, 23, 26,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  5, 15, 24, 32, 35, 44,  0,  0,  0])\n",
      "pos: tensor([ 7, 19, 25, 29, 30, 39, 45,  0,  0,  0])\n",
      "pos: tensor([ 1,  9, 22, 23, 29, 31,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 14, 23, 28, 36, 37, 39, 47,  0,  0])\n",
      "pos: tensor([ 2,  3,  4,  5, 12, 28, 36, 42, 50,  0])\n",
      "pos: tensor([ 2,  5, 17, 23, 24,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 23, 24, 25, 30, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  5,  8, 10, 16, 36, 41,  0,  0,  0])\n",
      "pos: tensor([ 2,  9, 11, 14, 29, 38, 44,  0,  0,  0])\n",
      "pos: tensor([ 9, 14, 22, 27, 30, 32, 41,  0,  0,  0])\n",
      "pos: tensor([ 5,  8, 24, 26, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  8, 14, 15, 21, 27, 34,  0,  0,  0])\n",
      "pos: tensor([ 5, 10, 11, 26, 37, 40, 45, 50,  0,  0])\n",
      "pos: tensor([ 4,  5, 21, 24, 37, 38, 41,  0,  0,  0])\n",
      "pos: tensor([ 2, 26, 28, 35, 38, 39, 47, 51,  0,  0])\n",
      "pos: tensor([ 8, 16, 20, 22, 26, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  3,  4,  9, 11, 24, 32, 38, 53,  0])\n",
      "pos: tensor([18, 25, 34, 35, 37, 38,  0,  0,  0,  0])\n",
      "pos: tensor([13, 14, 26, 29, 30, 38, 41, 46,  0,  0])\n",
      "pos: tensor([13, 19, 22, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  5,  9, 11, 12, 23, 25,  0,  0,  0])\n",
      "pos: tensor([ 5,  8,  9, 15,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  8, 11, 12, 27, 28, 34, 39,  0,  0])\n",
      "pos: tensor([22, 24, 26, 34, 38, 40, 41, 49,  0,  0])\n",
      "pos: tensor([ 5, 15, 19, 24, 27, 28, 40,  0,  0,  0])\n",
      "pos: tensor([11, 18, 24, 28, 30, 32, 40, 48,  0,  0])\n",
      "pos: tensor([ 5, 14, 16, 24, 32, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 30, 37, 39, 42, 46, 48, 49, 60,  0])\n",
      "pos: tensor([ 6, 12, 22, 26, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  3,  6, 23, 32, 39, 58, 59, 60, 61])\n",
      "pos: tensor([ 3,  7, 10, 17, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  8,  9, 10, 14, 19, 21, 36, 54,  0])\n",
      "pos: tensor([ 3, 11, 19, 30, 33, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  3,  8, 13, 22, 45, 52, 53,  0,  0])\n",
      "pos: tensor([15, 19, 25, 31, 32, 34, 40, 47,  0,  0])\n",
      "pos: tensor([ 3,  5, 14, 16, 23,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 12, 23, 24,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 10, 12, 16, 18, 20, 35,  0,  0,  0])\n",
      "pos: tensor([ 5, 11, 13, 24, 25, 26, 29, 46, 53,  0])\n",
      "pos: tensor([ 7,  9, 19, 26, 29, 37, 49, 54, 59,  0])\n",
      "pos: tensor([ 5, 11, 14, 17, 22, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 11, 15, 17, 28, 32, 43,  0,  0,  0])\n",
      "pos: tensor([ 4, 10, 14, 22, 25, 29, 33,  0,  0,  0])\n",
      "pos: tensor([ 1,  5,  7, 11, 27, 35, 41, 47, 48,  0])\n",
      "pos: tensor([ 6, 16, 17, 26, 31,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 15, 17, 18, 32, 33, 42, 50, 56,  0])\n",
      "pos: tensor([ 4, 11, 17,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([13, 16, 24, 26, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([10, 12, 13, 14, 17, 26, 35, 45,  0,  0])\n",
      "pos: tensor([14, 18, 20, 30, 40, 46, 49, 52,  0,  0])\n",
      "pos: tensor([ 5, 13, 28, 33, 36, 37, 39,  0,  0,  0])\n",
      "pos: tensor([ 6,  8, 11, 19, 20, 22, 23, 46, 50,  0])\n",
      "pos: tensor([ 4,  7, 28, 31, 34,  0,  0,  0,  0,  0])\n",
      "pos: tensor([10, 12, 15, 17, 22, 34,  0,  0,  0,  0])\n",
      "pos: tensor([1, 3, 6, 8, 0, 0, 0, 0, 0, 0])\n",
      "pos: tensor([ 5,  8, 10, 17, 19, 36, 38,  0,  0,  0])\n",
      "pos: tensor([ 2, 11, 12, 24, 25, 34, 38, 40,  0,  0])\n",
      "pos: tensor([12, 13, 19, 20, 36, 39, 42,  0,  0,  0])\n",
      "pos: tensor([ 2, 17, 28, 29, 31, 34, 35, 48,  0,  0])\n",
      "pos: tensor([ 8, 12, 23, 24, 26, 30,  0,  0,  0,  0])\n",
      "pos: tensor([10, 11, 19, 20, 28,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 10, 13, 15, 22, 26, 27, 46,  0,  0])\n",
      "pos: tensor([10, 11, 16, 17, 24, 25, 26, 38,  0,  0])\n",
      "pos: tensor([ 4, 13, 21, 34, 37, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  3, 15, 16, 21, 30,  0,  0,  0,  0])\n",
      "pos: tensor([19, 24, 27, 28, 30, 35, 38, 47,  0,  0])\n",
      "pos: tensor([ 2,  6, 13, 15, 17, 27, 43, 47, 48,  0])\n",
      "pos: tensor([ 2, 19, 24, 28, 45, 48, 49, 51, 57,  0])\n",
      "pos: tensor([ 9, 14, 23, 29, 36, 39, 44,  0,  0,  0])\n",
      "pos: tensor([ 1, 15, 20, 28, 36, 52, 53, 54, 56,  0])\n",
      "pos: tensor([12, 13, 18, 29, 32, 39, 44, 52, 58,  0])\n",
      "pos: tensor([ 7, 11, 16, 26, 27, 32, 40,  0,  0,  0])\n",
      "pos: tensor([ 1,  3, 18, 24, 28, 36, 43,  0,  0,  0])\n",
      "pos: tensor([ 2,  5, 12, 15, 25,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 16, 19, 27, 30, 40, 44, 50,  0,  0])\n",
      "pos: tensor([ 3, 16, 19, 25, 33, 37, 42,  0,  0,  0])\n",
      "pos: tensor([ 4,  7, 18, 22, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 13, 14, 19, 23, 33, 39, 44, 56,  0])\n",
      "pos: tensor([ 3, 10, 11,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 10, 14, 23, 42, 46,  0,  0,  0])\n",
      "pos: tensor([ 2, 22, 24, 26, 30, 33,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  8, 13,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  8,  9, 12,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  3,  9, 16, 18, 19, 30, 31, 44,  0])\n",
      "pos: tensor([ 5,  9, 14, 22, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  5,  9, 22, 26, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  4, 15, 32, 34, 38,  0,  0,  0,  0])\n",
      "pos: tensor([10, 13, 16, 21, 27, 33, 35,  0,  0,  0])\n",
      "pos: tensor([ 7, 13, 20, 36, 39, 43, 47,  0,  0,  0])\n",
      "pos: tensor([12, 17, 21, 27, 29, 41,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  9, 15, 20, 32, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 16, 20, 28, 33, 38, 42, 47,  0])\n",
      "pos: tensor([10, 14, 19, 20, 21, 34, 36,  0,  0,  0])\n",
      "pos: tensor([ 2, 14, 19, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 24, 28, 32, 40, 44,  0,  0,  0])\n",
      "pos: tensor([11, 24, 27, 28, 33, 35, 37,  0,  0,  0])\n",
      "pos: tensor([ 7, 19, 22, 31, 35, 37, 44, 55, 57,  0])\n",
      "pos: tensor([ 5, 20, 21, 22, 25, 26, 33, 49,  0,  0])\n",
      "pos: tensor([ 7, 10, 12, 15, 25, 36, 37,  0,  0,  0])\n",
      "pos: tensor([ 6, 12, 16, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 11, 26, 37, 45, 46, 48, 50, 52,  0])\n",
      "pos: tensor([ 6,  9, 14, 22, 24, 38, 41, 48,  0,  0])\n",
      "pos: tensor([ 5,  8, 16, 32, 34, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  6, 10, 23, 24, 25, 38,  0,  0,  0])\n",
      "pos: tensor([ 1,  5, 10, 18, 23, 27, 28,  0,  0,  0])\n",
      "pos: tensor([ 5, 11, 16, 17, 19, 22, 39, 42, 45,  0])\n",
      "pos: tensor([ 7,  9, 15, 16, 20, 39, 46, 50, 54,  0])\n",
      "pos: tensor([ 3,  8, 16, 22, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 14, 17, 19, 25, 39,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 20, 29, 30, 32, 33, 34, 42,  0,  0])\n",
      "pos: tensor([ 4,  8, 13, 27, 28, 31,  0,  0,  0,  0])\n",
      "pos: tensor([11, 13, 14, 21, 27, 42, 44, 47,  0,  0])\n",
      "pos: tensor([ 7,  9, 13, 26, 32, 39,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  8, 17, 27, 28,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 13, 16, 17, 18, 35, 43, 46, 48,  0])\n",
      "pos: tensor([ 6, 10, 19, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 10, 13, 19, 20, 26, 32, 49,  0,  0])\n",
      "pos: tensor([ 4, 15, 19, 20, 21, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 10, 15, 20, 39, 45,  0,  0,  0])\n",
      "pos: tensor([ 2,  3,  5, 11, 13, 18, 24, 33, 36,  0])\n",
      "pos: tensor([14, 20, 26, 27, 33,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 10, 30, 39, 41, 44, 51, 55, 57, 62])\n",
      "pos: tensor([ 1,  4,  5,  6, 15, 25, 45, 55, 60,  0])\n",
      "pos: tensor([ 3, 14, 15, 19, 21,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6,  9, 12, 13, 27, 33, 39,  0,  0,  0])\n",
      "pos: tensor([ 9, 16, 19, 32, 34, 36, 41,  0,  0,  0])\n",
      "pos: tensor([ 5, 11, 14, 19, 26, 27, 45, 46, 47,  0])\n",
      "pos: tensor([ 8, 18, 21, 31, 36, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  8, 19, 22, 26, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  9, 11, 15, 30, 36, 44, 45, 49,  0])\n",
      "pos: tensor([ 7, 14, 18, 36, 45, 47, 49, 50,  0,  0])\n",
      "pos: tensor([ 3,  6, 11, 19, 20, 22, 38, 48, 49, 57])\n",
      "pos: tensor([ 5, 16, 19, 21,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  6, 13, 22, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 12, 21, 22, 41, 43,  0,  0,  0])\n",
      "pos: tensor([ 1, 14, 22, 26, 30, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 6,  7, 10, 18, 20, 26, 32,  0,  0,  0])\n",
      "pos: tensor([11, 23, 24, 27, 33, 34, 35, 46, 54,  0])\n",
      "pos: tensor([ 1,  4,  8, 16, 26,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 19, 23, 32, 33, 38, 39, 41, 42, 52])\n",
      "pos: tensor([ 3, 17, 19, 24, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([15, 16, 17, 19, 20, 30, 47, 49,  0,  0])\n",
      "pos: tensor([ 1, 12, 16, 26, 28, 31, 34,  0,  0,  0])\n",
      "pos: tensor([ 1,  4, 16, 20, 28, 29, 32,  0,  0,  0])\n",
      "pos: tensor([ 9, 11, 15, 21, 26, 29,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  7,  9, 10, 14, 17, 35, 38,  0,  0])\n",
      "pos: tensor([ 2,  3,  5,  8, 16, 17, 28, 32,  0,  0])\n",
      "pos: tensor([ 7, 12, 20, 25, 27, 35,  0,  0,  0,  0])\n",
      "pos: tensor([16, 18, 23, 24, 27, 41, 44, 54, 56,  0])\n",
      "pos: tensor([ 6,  8, 21, 26, 31, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  4,  6, 16, 24, 28, 29, 37, 54,  0])\n",
      "pos: tensor([ 4, 10, 15, 16, 18, 22, 32, 46, 56,  0])\n",
      "pos: tensor([28, 31, 32, 33, 41, 53, 55, 56, 58,  0])\n",
      "pos: tensor([ 3,  9, 14, 18,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 13, 23, 24, 26, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  7, 14, 16, 24, 36, 37,  0,  0,  0])\n",
      "pos: tensor([14, 26, 30, 33, 38, 40,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 19, 25, 28, 32, 33, 34, 42, 61, 62])\n",
      "pos: tensor([ 4,  5, 19, 26, 29, 43, 45, 47,  0,  0])\n",
      "pos: tensor([ 4, 10, 15, 26, 27, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  7,  8, 18, 21, 35, 39, 40,  0,  0])\n",
      "pos: tensor([ 4,  6, 10, 16, 28, 39, 43,  0,  0,  0])\n",
      "pos: tensor([ 1,  2, 10, 28, 29, 39, 40,  0,  0,  0])\n",
      "pos: tensor([10, 15, 19, 20,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 12, 14, 15, 25, 27, 41, 43, 57,  0])\n",
      "pos: tensor([ 2, 13, 19, 23, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 14, 17, 21, 40, 48, 51,  0,  0])\n",
      "pos: tensor([ 3, 11, 12, 20,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 14, 20, 35, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 11, 12, 25,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 14, 21, 28, 31, 39, 44, 48, 56, 60])\n",
      "pos: tensor([ 1,  2,  4, 22, 29, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  8,  9, 13, 15, 19, 40,  0,  0,  0])\n",
      "pos: tensor([12, 17, 21, 23, 30, 31,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 16, 26, 30, 33, 41, 45, 54,  0,  0])\n",
      "pos: tensor([ 1,  9, 22, 35, 36, 39, 43, 50, 56,  0])\n",
      "pos: tensor([ 4, 10, 13, 19, 28, 39, 40,  0,  0,  0])\n",
      "pos: tensor([ 7, 12, 17, 21, 29, 30, 33, 43,  0,  0])\n",
      "pos: tensor([ 4, 16, 19, 22, 37, 40, 47, 56, 60,  0])\n",
      "pos: tensor([ 4, 12, 18, 21, 28, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 11, 12, 16, 22, 27, 35, 36, 54,  0])\n",
      "pos: tensor([ 9, 10, 14, 15, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 14, 20, 22, 26, 30, 32, 38, 49,  0])\n",
      "pos: tensor([ 1, 11, 20, 24, 28, 32, 41,  0,  0,  0])\n",
      "pos: tensor([ 4, 13, 15, 21, 24, 40, 45,  0,  0,  0])\n",
      "pos: tensor([11, 27, 28, 31, 36, 38, 42, 52,  0,  0])\n",
      "pos: tensor([ 1,  3, 12, 16, 39, 42, 50, 54, 55,  0])\n",
      "pos: tensor([13, 18, 20, 41, 43, 44, 46, 47, 49,  0])\n",
      "pos: tensor([ 5,  7,  9, 11, 27, 37, 42, 44,  0,  0])\n",
      "pos: tensor([ 6, 15, 20, 28, 29, 42, 44, 47, 48,  0])\n",
      "pos: tensor([13, 14, 16, 20, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([12, 18, 30, 36, 38, 39, 42, 43,  0,  0])\n",
      "pos: tensor([ 3, 10, 16, 30, 33, 45, 46, 49,  0,  0])\n",
      "pos: tensor([ 1,  2,  4,  8, 14, 16, 21, 50,  0,  0])\n",
      "pos: tensor([ 7, 10, 11, 21, 28, 31, 36,  0,  0,  0])\n",
      "pos: tensor([ 3, 20, 21, 23, 25, 29, 33, 43, 58,  0])\n",
      "pos: tensor([ 1,  3,  7, 16, 18, 34, 40,  0,  0,  0])\n",
      "pos: tensor([ 9, 16, 21, 22, 25, 28,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  4,  5, 21,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 19, 26,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([13, 17, 20, 26, 27, 34, 36,  0,  0,  0])\n",
      "pos: tensor([ 3,  9, 14, 15, 26, 29, 30, 38,  0,  0])\n",
      "pos: tensor([ 5,  6,  7, 28, 33, 39,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 14, 18, 29, 31, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  5, 11, 25, 40, 42, 44,  0,  0,  0])\n",
      "pos: tensor([ 7, 16, 23, 33, 41, 42, 43, 47,  0,  0])\n",
      "pos: tensor([10, 11, 14, 27, 30,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  2, 12, 27, 34, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 16, 23, 24, 29, 35, 43, 54, 55,  0])\n",
      "pos: tensor([ 7,  8, 10, 14, 25, 33, 39, 42, 52,  0])\n",
      "pos: tensor([16, 19, 21, 22,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 14, 15, 17, 28, 38, 47, 49, 56,  0])\n",
      "pos: tensor([14, 24, 27, 30, 34, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  7, 12, 29, 38, 41,  0,  0,  0,  0])\n",
      "pos: tensor([14, 16, 17, 22, 23, 34, 38,  0,  0,  0])\n",
      "pos: tensor([ 9, 11, 17, 23, 24, 32, 43, 50, 61, 62])\n",
      "pos: tensor([ 2, 10, 15, 22, 30, 33, 36, 37, 46,  0])\n",
      "pos: tensor([ 8, 16, 22, 26, 29, 33,  0,  0,  0,  0])\n",
      "pos: tensor([10, 13, 24, 30, 33, 34, 44,  0,  0,  0])\n",
      "pos: tensor([ 2, 15, 23, 28,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 13, 20, 32, 34, 36, 37, 47,  0,  0])\n",
      "pos: tensor([ 1,  4,  7, 10, 14, 27,  0,  0,  0,  0])\n",
      "pos: tensor([17, 18, 21, 22, 25, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  2,  6, 10, 13, 19, 21, 45,  0,  0])\n",
      "pos: tensor([21, 23, 24, 29, 37, 39, 49, 50,  0,  0])\n",
      "pos: tensor([ 1, 14, 28, 29, 31,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  4, 14, 32, 36, 37, 43,  0,  0,  0])\n",
      "pos: tensor([ 8, 13, 16, 27, 29, 31,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  6,  8, 12, 19, 29,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 13, 19, 21,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  7, 14, 21, 23, 27, 29, 39, 51,  0])\n",
      "pos: tensor([ 1,  4,  9, 10, 33, 40, 41, 45,  0,  0])\n",
      "pos: tensor([ 4, 16, 17, 34, 41, 44, 49, 51,  0,  0])\n",
      "pos: tensor([ 9, 11, 20, 29, 30, 31,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 24, 28, 33, 35, 36, 46,  0,  0,  0])\n",
      "pos: tensor([ 3, 13, 15, 16, 19, 26, 28, 31,  0,  0])\n",
      "pos: tensor([ 9, 23, 24, 25, 33, 36, 38,  0,  0,  0])\n",
      "pos: tensor([15, 23, 29, 31, 38, 44, 50, 51,  0,  0])\n",
      "pos: tensor([12, 19, 20, 24, 29, 32, 37,  0,  0,  0])\n",
      "pos: tensor([ 1,  8, 16, 23, 28,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  8, 25, 33, 35, 38,  0,  0,  0,  0])\n",
      "pos: tensor([15, 25, 33, 37, 40, 42, 43, 49,  0,  0])\n",
      "pos: tensor([ 1, 13, 16, 29, 32, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 19, 29, 32, 40, 42, 45, 48,  0,  0])\n",
      "pos: tensor([ 1,  8, 10, 18, 29, 30, 31, 41,  0,  0])\n",
      "pos: tensor([ 8, 10, 11, 23, 27, 40, 43, 53, 54, 59])\n",
      "pos: tensor([10, 18, 19, 26, 31,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8,  9, 28, 31, 32, 33, 39, 44, 50,  0])\n",
      "pos: tensor([ 1,  7, 10, 20, 21, 23, 39, 43, 47,  0])\n",
      "pos: tensor([ 9, 10, 17, 22, 31,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 21, 28, 36, 42, 44, 45, 50,  0,  0])\n",
      "pos: tensor([ 1,  9, 11, 19, 25, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 14, 17, 18,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  2, 10, 14, 15, 23, 36, 37, 52,  0])\n",
      "pos: tensor([ 2, 12, 13, 14,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6,  8, 10, 19, 20, 28,  0,  0,  0,  0])\n",
      "pos: tensor([11, 12, 13, 22, 28, 29,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  7, 27, 28, 31, 38, 41,  0,  0,  0])\n",
      "pos: tensor([ 1,  5, 12, 21, 38, 39, 43,  0,  0,  0])\n",
      "pos: tensor([11, 13, 25, 28, 39, 44, 45, 47,  0,  0])\n",
      "pos: tensor([ 2,  6,  9, 10, 14, 20, 25, 43, 51,  0])\n",
      "pos: tensor([ 6, 18, 26, 31, 34, 41, 49, 50,  0,  0])\n",
      "pos: tensor([ 5, 11, 18, 19,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 12, 16, 18, 20, 34, 38, 39,  0,  0])\n",
      "pos: tensor([ 1,  5,  7, 11, 18, 38,  0,  0,  0,  0])\n",
      "pos: tensor([10, 18, 20, 32, 33, 36, 44, 48, 49,  0])\n",
      "pos: tensor([ 9, 11, 12, 27, 30, 33, 37, 52,  0,  0])\n",
      "pos: tensor([ 2,  9, 19, 21, 32, 33, 38, 48, 60,  0])\n",
      "pos: tensor([13, 14, 19, 23, 28, 33, 36, 53,  0,  0])\n",
      "pos: tensor([ 4,  9, 14, 16, 20, 24, 38, 49,  0,  0])\n",
      "pos: tensor([ 3,  6, 16, 19, 25, 31, 45,  0,  0,  0])\n",
      "pos: tensor([ 1,  2,  4,  6, 18, 27, 43,  0,  0,  0])\n",
      "pos: tensor([14, 23, 27, 34, 42, 44, 45, 46,  0,  0])\n",
      "pos: tensor([ 8, 15, 24, 25, 35, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 15, 17, 21,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 16, 18, 19, 25, 30, 39, 48,  0,  0])\n",
      "pos: tensor([12, 16, 20, 21, 27, 30,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 12, 16, 35, 40, 43, 46, 49,  0,  0])\n",
      "pos: tensor([17, 19, 24, 25, 33, 37, 42,  0,  0,  0])\n",
      "pos: tensor([ 2,  6, 12, 34, 37, 38, 44, 46, 55,  0])\n",
      "pos: tensor([22, 23, 26, 28, 34, 38, 39, 46,  0,  0])\n",
      "pos: tensor([ 5, 11, 37, 38, 45, 47, 56, 58, 59,  0])\n",
      "pos: tensor([ 3,  6,  7, 15, 18, 24, 35, 46,  0,  0])\n",
      "pos: tensor([ 2,  7,  9, 13, 18, 28, 36, 38, 47,  0])\n",
      "pos: tensor([ 2,  4,  9, 25, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 10, 27, 32, 35, 36, 38, 47,  0,  0])\n",
      "pos: tensor([ 1,  9, 17, 22, 25, 31, 36,  0,  0,  0])\n",
      "pos: tensor([ 1,  8, 11, 14, 18, 30, 34,  0,  0,  0])\n",
      "pos: tensor([ 4, 15, 19, 20,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  9, 10, 15, 16, 30, 45, 49,  0,  0])\n",
      "pos: tensor([ 4, 11, 34, 35, 50, 52, 54, 56, 58,  0])\n",
      "pos: tensor([ 7, 13, 15, 19, 22, 33, 39,  0,  0,  0])\n",
      "pos: tensor([ 3, 12, 19, 25, 29, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 17, 21, 24, 39, 40, 43, 48,  0,  0])\n",
      "pos: tensor([ 2, 15, 18, 20, 35, 36,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 16, 34, 37, 38, 39, 43, 44,  0,  0])\n",
      "pos: tensor([ 6, 12,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 13, 16, 31, 32,  0,  0,  0,  0,  0])\n",
      "pos: tensor([11, 16, 22, 23, 29, 39, 40,  0,  0,  0])\n",
      "pos: tensor([1, 6, 7, 0, 0, 0, 0, 0, 0, 0])\n",
      "pos: tensor([ 6, 14, 15, 16, 26, 38, 44, 46, 50,  0])\n",
      "pos: tensor([12, 14, 21, 22, 23, 24, 27,  0,  0,  0])\n",
      "pos: tensor([ 4,  5, 16, 17, 32,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  4,  6, 14, 33, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  6, 18, 19,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 11, 17, 25, 27,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 6,  7, 22, 24, 25, 33, 42,  0,  0,  0])\n",
      "pos: tensor([12, 14, 29, 32, 35, 42, 43, 50,  0,  0])\n",
      "pos: tensor([ 6, 10, 11, 15, 37, 41, 42,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 14, 19, 29, 37, 42,  0,  0,  0])\n",
      "pos: tensor([13, 18, 24, 33, 34, 42, 47,  0,  0,  0])\n",
      "pos: tensor([ 3,  4,  8, 13, 20, 30, 38, 44,  0,  0])\n",
      "pos: tensor([ 7,  9, 11, 19, 20, 31, 34, 35,  0,  0])\n",
      "pos: tensor([13, 14, 23, 29, 42, 46, 53, 59, 61,  0])\n",
      "pos: tensor([2, 4, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pos: tensor([ 4, 16, 18, 32, 33, 37, 45, 47, 55,  0])\n",
      "pos: tensor([ 2, 11, 17, 22, 34, 39,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  4,  5,  8, 19, 24, 35, 49,  0,  0])\n",
      "pos: tensor([ 1,  2,  3, 10, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8,  9, 12, 15, 22, 27, 31, 32, 55,  0])\n",
      "pos: tensor([ 9, 11, 18, 23, 24, 31, 35, 46,  0,  0])\n",
      "pos: tensor([ 3,  7,  9, 13, 14, 26, 33,  0,  0,  0])\n",
      "pos: tensor([ 6,  7, 13, 26, 27, 29, 44,  0,  0,  0])\n",
      "pos: tensor([ 1, 11, 17,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 14, 19, 25, 27, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 17, 29, 30, 33, 36, 45, 46, 49,  0])\n",
      "pos: tensor([11, 21, 28, 30, 34, 38,  0,  0,  0,  0])\n",
      "pos: tensor([ 7,  8, 18,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 17, 31, 38, 39, 45, 47, 50,  0,  0])\n",
      "pos: tensor([3, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pos: tensor([ 8,  9, 22, 25,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 14, 20, 25, 27, 29, 42, 44,  0,  0])\n",
      "pos: tensor([21, 22, 23, 27, 36, 41, 43, 46, 54,  0])\n",
      "pos: tensor([ 1,  6, 16, 19,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 11, 17, 24, 25, 31,  0,  0,  0,  0])\n",
      "pos: tensor([10, 13, 14, 22, 33, 37, 38, 43, 55,  0])\n",
      "pos: tensor([ 3,  6,  9, 10, 14, 19, 42, 44,  0,  0])\n",
      "pos: tensor([11, 19, 22, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  4,  9, 26, 35, 38, 42,  0,  0,  0])\n",
      "pos: tensor([ 4, 10, 21, 25, 29, 34,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 10, 17, 20, 36, 52, 54, 55, 57,  0])\n",
      "pos: tensor([ 5, 11, 15, 21, 29, 31,  0,  0,  0,  0])\n",
      "pos: tensor([12, 17, 18, 21, 24, 30,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 13, 20, 25, 40, 43, 49, 52, 57,  0])\n",
      "pos: tensor([ 2,  4,  6, 23, 31, 35,  0,  0,  0,  0])\n",
      "pos: tensor([ 5,  9, 12,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 8, 21, 24, 28, 31, 41, 45, 49, 55,  0])\n",
      "pos: tensor([ 2, 12, 14, 21,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 14, 17, 18, 23, 26, 34,  0,  0,  0])\n",
      "pos: tensor([ 8, 10, 13, 16, 20, 41, 43,  0,  0,  0])\n",
      "pos: tensor([ 3,  4,  9, 16, 32, 41, 44, 46,  0,  0])\n",
      "pos: tensor([ 4,  7, 18, 25, 26, 35, 44,  0,  0,  0])\n",
      "pos: tensor([ 5,  7,  8, 20, 35, 39, 42, 59, 60,  0])\n",
      "pos: tensor([ 8, 15, 25, 26, 28, 31, 45,  0,  0,  0])\n",
      "pos: tensor([ 3, 10, 17, 20, 25,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2,  5,  8, 13, 31, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 3, 11, 15, 20, 21, 22, 23, 28, 33,  0])\n",
      "pos: tensor([ 1, 12, 15, 22, 24, 26, 28, 32, 47,  0])\n",
      "pos: tensor([ 3,  9, 22, 25, 28,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  6, 19, 20,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 10, 17, 27, 29,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  4,  7,  8, 26, 40, 43,  0,  0,  0])\n",
      "pos: tensor([ 6, 17, 26, 28, 33, 36, 45, 48,  0,  0])\n",
      "pos: tensor([11, 12, 13, 21, 23, 28, 36, 46,  0,  0])\n",
      "pos: tensor([ 9, 10, 11, 16,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([17, 20, 27, 31, 33,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  5, 10, 28, 30, 39, 43, 44,  0,  0])\n",
      "pos: tensor([ 2, 10, 21, 23, 24,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  5, 11, 15, 23, 40, 44,  0,  0,  0])\n",
      "pos: tensor([ 2, 13, 21,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4, 14, 19, 24, 31, 33, 34, 49, 53,  0])\n",
      "pos: tensor([ 1,  5, 13, 14, 17,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  9, 14, 21, 27, 39, 40,  0,  0,  0])\n",
      "pos: tensor([ 5, 21, 29, 33, 35, 41, 44,  0,  0,  0])\n",
      "pos: tensor([ 2,  5, 18, 19, 20, 26, 35,  0,  0,  0])\n",
      "pos: tensor([ 4, 10, 18, 30, 32, 35, 44, 47, 59,  0])\n",
      "pos: tensor([ 2, 11, 15,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  7, 18, 19, 21, 42, 45,  0,  0,  0])\n",
      "pos: tensor([ 4,  7, 10, 22, 28, 31,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  2,  6, 17, 27, 31, 37, 40,  0,  0])\n",
      "pos: tensor([ 9, 15, 19, 22,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  9, 10, 12, 36, 38, 41, 43, 52,  0])\n",
      "pos: tensor([ 4,  9, 10, 14, 42, 46, 47, 51, 61,  0])\n",
      "pos: tensor([ 1,  3, 14, 25, 26, 28, 33, 41, 51,  0])\n",
      "pos: tensor([ 7,  9, 20, 28, 29, 37, 39, 40,  0,  0])\n",
      "pos: tensor([13, 15, 24, 26, 31, 34, 39,  0,  0,  0])\n",
      "pos: tensor([ 2,  3,  4,  6,  7, 17, 20, 33,  0,  0])\n",
      "pos: tensor([6, 8, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "pos: tensor([ 1,  2, 20, 22, 26, 37,  0,  0,  0,  0])\n",
      "pos: tensor([ 6, 13, 14, 15, 24,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 9, 18, 34, 41, 42, 46, 50, 53, 56,  0])\n",
      "pos: tensor([18, 20, 22, 33, 35, 37, 41,  0,  0,  0])\n",
      "pos: tensor([ 7, 26, 28, 32, 38, 40, 47, 48, 50,  0])\n",
      "pos: tensor([ 1,  9, 11, 18, 35, 37, 39, 41,  0,  0])\n",
      "pos: tensor([ 7, 20, 23, 25, 43, 46, 47, 51, 52,  0])\n",
      "pos: tensor([ 2,  7, 14, 22, 27, 28,  0,  0,  0,  0])\n",
      "pos: tensor([ 1,  8, 10, 15, 17, 19, 30, 36,  0,  0])\n",
      "pos: tensor([12, 19, 20, 21, 35, 36, 41,  0,  0,  0])\n",
      "pos: tensor([ 3,  4,  9, 11, 16, 17, 19, 30,  0,  0])\n",
      "pos: tensor([ 1,  8, 27, 28, 34, 42, 47,  0,  0,  0])\n",
      "pos: tensor([ 7,  9, 16, 22, 29, 33,  0,  0,  0,  0])\n",
      "pos: tensor([ 3,  7, 10, 17, 26, 28, 30, 42,  0,  0])\n",
      "pos: tensor([14, 16, 20, 31, 36, 40, 45,  0,  0,  0])\n",
      "pos: tensor([ 4, 15, 18, 22, 30, 48, 49, 51, 52,  0])\n",
      "pos: tensor([ 9, 16, 17, 20, 23, 33,  0,  0,  0,  0])\n",
      "pos: tensor([ 1, 12, 13, 16, 17, 18, 45,  0,  0,  0])\n",
      "pos: tensor([ 7,  9, 13, 17,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 4,  7,  9, 25, 31, 32,  0,  0,  0,  0])\n",
      "pos: tensor([ 5, 16, 17, 19, 20, 32, 37, 46, 50,  0])\n",
      "pos: tensor([18, 19, 23, 24,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 2, 11, 14,  0,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([12, 17, 22, 29, 41, 43, 46,  0,  0,  0])\n",
      "pos: tensor([ 7, 10, 14, 30, 39, 42, 47, 52, 53,  0])\n",
      "pos: tensor([ 2, 10, 14, 21, 28, 31, 38,  0,  0,  0])\n",
      "pos: tensor([ 1,  9, 21, 24, 25,  0,  0,  0,  0,  0])\n",
      "pos: tensor([10, 16, 22, 24, 37, 38, 43,  0,  0,  0])\n",
      "pos: tensor([ 1,  8, 14, 16, 37, 53, 54, 56, 60,  0])\n",
      "pos: tensor([ 2, 20, 21, 23,  0,  0,  0,  0,  0,  0])\n",
      "pos: tensor([ 7, 12, 13, 19, 21, 24, 44, 46, 54,  0])\n",
      "pos: tensor([ 6,  7, 10, 12, 16, 24, 39,  0,  0,  0])\n",
      "pos: tensor([ 3,  5,  7, 13, 16, 22, 38, 44,  0,  0])\n",
      "pos: tensor([ 1,  2,  3,  6, 10, 35, 42, 49,  0,  0])\n",
      "pos: tensor([10, 11, 15, 19, 32, 34, 41, 50, 57,  0])\n",
      "pos: tensor([ 5,  6, 15, 21, 23, 45, 46, 48,  0,  0])\n",
      "pos: tensor([ 9, 15, 17, 19, 24, 30, 33, 43, 54,  0])\n",
      "pos: tensor([ 7,  8, 10, 13, 26, 34, 42, 47, 48,  0])\n",
      "pos: tensor([ 8, 11, 15, 16, 27,  0,  0,  0,  0,  0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(f\"{tokens_X.shape}\\n\"\n",
    "          f\"{segments_X.shape}\\n\"\n",
    "          f\"{valid_lens_x.shape}\\n\"\n",
    "          f\"{pred_positions_X.shape}\\n\"\n",
    "          f\"{mlm_weights_X.shape}\\n\"\n",
    "          f\"{mlm_Y.shape}\\n\"\n",
    "          f\"{nsp_y.shape}\")\n",
    "    \n",
    "\n",
    "    print(f\"{tokens_X}\\n\"\n",
    "          f\"{segments_X}\\n\"\n",
    "          f\"{valid_lens_x}\\n\"\n",
    "          f\"{pred_positions_X}\\n\"\n",
    "          f\"{mlm_weights_X}\\n\"\n",
    "          f\"{mlm_Y}\\n\"\n",
    "          f\"{nsp_y}\")\n",
    "\n",
    "    \n",
    "      \n",
    "    for pos in pred_positions_X:\n",
    "          print(f\"pos: {pos}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b78dd7",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "最后，我们来看一下词量。即使在过滤掉不频繁的词元之后，它仍然比PTB数据集的大两倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47b86684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:52.159404Z",
     "iopub.status.busy": "2023-08-18T07:00:52.158958Z",
     "iopub.status.idle": "2023-08-18T07:00:52.169643Z",
     "shell.execute_reply": "2023-08-18T07:00:52.168438Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081adbe2",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 与PTB数据集相比，WikiText-2数据集保留了原来的标点符号、大小写和数字，并且比PTB数据集大了两倍多。\n",
    "* 我们可以任意访问从WikiText-2语料库中的一对句子生成的预训练（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：`pip install nltk`。在代码中先`import nltk`。然后下载Punkt语句词元分析器：`nltk.download('punkt')`。要拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用`nltk.tokenize.sent_tokenize(sentences)`将返回两个句子字符串的列表：`['This is great !', 'Why not ?']`。\n",
    "1. 如果我们不过滤出一些不常见的词元，词量会有多大？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcf3ae",
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/5738)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
