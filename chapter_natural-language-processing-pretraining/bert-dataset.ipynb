{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6875f27",
   "metadata": {
    "origin_pos": 0
   },
   "source": [
    "# 用于预训练BERT的数据集\n",
    ":label:`sec_bert-dataset`\n",
    "\n",
    "为了预训练 :numref:`sec_bert`中实现的BERT模型，我们需要以理想的格式生成数据集，以便于两个预训练任务：遮蔽语言模型和下一句预测。一方面，最初的BERT模型是在两个庞大的图书语料库和英语维基百科（参见 :numref:`subsec_bert_pretraining_tasks`）的合集上预训练的，但它很难吸引这本书的大多数读者。另一方面，现成的预训练BERT模型可能不适合医学等特定领域的应用。因此，在定制的数据集上对BERT进行预训练变得越来越流行。为了方便BERT预训练的演示，我们使用了较小的语料库WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`。\n",
    "\n",
    "与 :numref:`sec_word2vec_data`中用于预训练word2vec的PTB数据集相比，WikiText-2（1）保留了原来的标点符号，适合于下一句预测；（2）保留了原来的大小写和数字；（3）大了一倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "342b7589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:38.284931Z",
     "iopub.status.busy": "2023-08-18T07:00:38.284353Z",
     "iopub.status.idle": "2023-08-18T07:00:41.113963Z",
     "shell.execute_reply": "2023-08-18T07:00:41.112838Z"
    },
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691a2248",
   "metadata": {
    "origin_pos": 4
   },
   "source": [
    "在WikiText-2数据集中，每行代表一个段落，其中在任意标点符号及其前面的词元之间插入空格。保留至少有两句话的段落。为了简单起见，我们仅使用句号作为分隔符来拆分句子。我们将更复杂的句子拆分技术的讨论留在本节末尾的练习中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb911790",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.118878Z",
     "iopub.status.busy": "2023-08-18T07:00:41.118515Z",
     "iopub.status.idle": "2023-08-18T07:00:41.124582Z",
     "shell.execute_reply": "2023-08-18T07:00:41.123696Z"
    },
    "origin_pos": 5,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "def load_wikitext_2():\n",
    "    # 从 Hugging Face 下载 wikitext-2 数据集\n",
    "    # 数据集大小：12.1 MB\n",
    "    wikitext2 = load_dataset(\"wikitext\", name=\"wikitext-2-v1\", cache_dir=\"D:\\Github\\AI\\d2l-pytorch\\data\")\n",
    "    # 拆分数据集\n",
    "    train_iter = wikitext2[\"train\"]\n",
    "    valid_iter = wikitext2[\"validation\"]\n",
    "    test_iter = wikitext2[\"test\"]\n",
    "    return train_iter, valid_iter, test_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2ddb400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_wiki():\n",
    "    train_iter, _, _ = load_wikitext_2()\n",
    "    # 将数据集转换为列表\n",
    "    # lines:List[Str]\n",
    "    lines = [line['text'] for line in train_iter]\n",
    "    # paragraphs:List[List[Str]]\n",
    "    # 只有当你对字符串主动调用会返回列表的方法时，才会产生子列表,比如这里的split()\n",
    "    paragraphs = [line.strip().lower().split(' . ') for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "    # paragraphs:List[List[Str]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9e18ab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jacob anthony degrom ( born june 19 , 1988 ) , is an american professional baseball pitcher for the new york mets of major league baseball ( mlb )', 'prior to playing professionally , degrom attended stetson university and played college baseball for the stetson hatters .']\n",
      "['the notion that vikings used a. muscaria to produce their <unk> <unk> was first suggested by the swedish professor samuel <unk> in 1784', '<unk> based his theories on reports about the use of fly agaric among siberian shamans', 'the notion has become widespread since the 19th century , but no contemporary sources mention this use or anything similar in their description of <unk>', 'muscimol is generally a mild <unk> , but it can create a range of different reactions within a group of people', 'it is possible that it could make a person angry , or cause them to be \" very <unk> or sad , jump about , dance , sing or give way to great <unk> \" .']\n",
      "[\"the day after the young turks ' press conference , taylor privately met khánh at the latter 's office\", 'he complained about the dissolution of the hnc and said it did not accord with the values of the alliance and the loyalty washington expected of saigon', 'he added that the us could not cooperate with two governments at once : a military regime that held power while a civilian body took the responsibility', 'khánh <unk> replied that vietnam was not a satellite of the us and compared the situation to the us support of the successful coup against diệm , saying that loyalty was meant to be <unk>', 'khánh had hinted that he felt the americans were about to have him deposed like diệm , who was then assassinated , but this rankled taylor , who had argued against the regime change', 'taylor then <unk> khánh , saying he had lost confidence in the vietnamese officer , recommending khánh resign and go into exile', 'he also said military supplies currently being shipped to vietnam would be withheld after arriving in saigon and that american help in planning and advising military operations would be suspended .']\n",
      "['the 1986 peach bowl kicked off five years minus one day since virginia tech had last played in atlanta — during the 1981 peach bowl', \"virginia tech scored first in the game , but nc state 's <unk> blocked a tech punt in the tech end zone and recovered it for a tying touchdown\", 'virginia tech kicked a field goal at the end of the quarter to take a 10 – 7 lead , but nc state fought back , scoring 14 <unk> points in the second quarter to take a 21 – 10 lead by halftime', 'in the third quarter , the game turned into a defensive battle', 'neither side scored until late in the third quarter , when tech took advantage of a state fumble to score the first touchdown of the second half', 'tech failed to convert a two @-@ point conversion , but nc state fumbled again on the ensuing possession , and tech was able to drive for another touchdown', 'leading 22 – 21 , tech attempted another two @-@ point conversion , which also failed .']\n",
      "['\" don \\'t you wanna stay \" was covered by colton dixon and <unk> <unk> in the eleventh season of american idol', 'natalie finn of e ! gave a mixed review of the pair \\'s performance , writing \" <unk> handled kelly clarkson better than colton played jason aldean on \" don \\'t you wanna stay , \" but she \\'s the country girl , so it made sense', '\" brian mansfield of usa today felt that the song was out of dixon \\'s comfort zone and a little out of <unk> \\'s range', 'gil kaufman of mtv remarked that the chemistry between the pair was more like cold fusion', 'jennifer still of digital spy said the performance \" isn \\'t anything incredible \" .']\n",
      "['this episode as well as the entire \" road to \" series in family guy is a parody of the seven road to ... comedy films which were released from 1940 to 1962 , starring actors bing crosby , bob hope and actress dorothy <unk>', 'the opening credits show images with brian and stewie referencing other christmas <unk> such as the <unk> , a christmas carol , how the <unk> <unk> christmas , <unk> the <unk> and home alone', \"the credits also show brian and stewie performing winter activities , such as <unk> fights , making snow angels and putting coal in the ( meg 's ) christmas socks instead of gifts\", 'ron macfarlane , who narrated part of the episode , mentioned that kenny rogers was supposed to be there .']\n",
      "['however , the germans launched a counterattack at 03 : 15 on 16 december , throwing in men from the 6th parachute regiment , sent by herr to the 26th panzer division to relieve the exhausted 9th panzergrenadier regiment', 'these troops had arrived late that evening after a long journey', '<unk> by tanks , they attacked the right @-@ hand new zealand positions held by the 21st nz battalion , but were held off and had retired by daylight', 'meanwhile , even before the german counterattack had been repelled , the 20th regiment had attacked toward orsogna with two squadrons of sherman tanks', 'under intense artillery and anti @-@ tank fire , the tanks and infantry became separated and the tanks became a target rather than a threat .']\n",
      "['in hyderabad , two rival mafia gangs headed by dubai @-@ based don ali bhai , and narayana resort to criminal activities such as <unk> , murder , and <unk> for various reasons', 'the new commissioner of police , <unk> <unk> pasha qadri , focuses on making the city a better place by working at arresting all of them', \"pandu , a <unk> gangster living in hyderabad along with his friends , is hired by narayana and attacks ali bhai 's henchmen\", \"he later joins ali bhai 's gang for monetary reasons\", 'he falls in love with shruti , an aerobics teacher , who rejects his advances .']\n",
      "['boise state was undefeated in conference play going into the final game of the regular season , and suffered just one out @-@ of @-@ conference loss', 'in that final game , bsu lost 39 – 27 to undefeated hawaiʻi', \"following the game , the broncos decided to travel to hawaiʻi , rather than play at home in the humanitarian bowl , which is located at boise 's home stadium in boise , idaho and features a matchup between a wac team and one from the atlantic coast conference\", 'boise ended its regular season 10 – 2 overall and 7 – 1 in conference play .']\n",
      "[\"bédard planned to leave polytron after finishing fez to experience work with a full development team , but stayed to port the windows release before joining toronto 's <unk> games\", \"he credited the game 's long development cycle to his own inexperience in game development ( compounded by the team 's small size and difficulty in setting reasonable milestones ) , the game 's scope , and fish 's <unk>\", \"fish had hoped that players would discuss fez 's nuances online after the game 's release\", 'players collaborated online for a week to solve the final \" <unk> \" puzzle by brute force', 'ars technica described the apparent end to the game \\'s harder puzzles as \" <unk> \" , but fish told eurogamer in march 2013 that hidden in @-@ game secrets remain to be found .']\n"
     ]
    }
   ],
   "source": [
    "paragraphs = _read_wiki()\n",
    "# for 行内所有句子 in 行\n",
    "for paragraph in paragraphs[:10]:\n",
    "    print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f5515b",
   "metadata": {
    "origin_pos": 6
   },
   "source": [
    "## 为预训练任务定义辅助函数\n",
    "\n",
    "在下文中，我们首先为BERT的两个预训练任务实现辅助函数。这些辅助函数将在稍后将原始文本语料库转换为理想格式的数据集时调用，以预训练BERT。\n",
    "\n",
    "### 生成下一句预测任务的数据\n",
    "\n",
    "根据 :numref:`subsec_nsp`的描述，`_get_next_sentence`函数生成二分类任务的训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "246ca273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.128645Z",
     "iopub.status.busy": "2023-08-18T07:00:41.128375Z",
     "iopub.status.idle": "2023-08-18T07:00:41.133471Z",
     "shell.execute_reply": "2023-08-18T07:00:41.132347Z"
    },
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# sentence: 当前句子\n",
    "# next_sentence: 下一个句子\n",
    "# paragraphs: 所有行\n",
    "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    # 抛硬币决定是否是下一个句子\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        # 替换掉下一句\n",
    "        # 随机选择一行，然后随机选择一个句子\n",
    "        # 有可能选到当前句子或者当前句子的下一句，这个算噪声\n",
    "        # paragraph:List[List[Str]]\n",
    "        next_sentence = random.choice(random.choice(paragraphs))\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b1d432",
   "metadata": {
    "origin_pos": 8
   },
   "source": [
    "下面的函数通过调用`_get_next_sentence`函数从输入`paragraph`生成用于下一句预测的训练样本。这里`paragraph`是句子列表，其中每个句子都是词元列表。自变量`max_len`指定预训练期间的BERT输入序列的最大长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7686fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.137934Z",
     "iopub.status.busy": "2023-08-18T07:00:41.137439Z",
     "iopub.status.idle": "2023-08-18T07:00:41.143146Z",
     "shell.execute_reply": "2023-08-18T07:00:41.142265Z"
    },
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
    "    nsp_data_from_paragraph = []\n",
    "    # 遍历1行除了最后一个句子外的所有句子\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = _get_next_sentence(paragraph[i], paragraph[i + 1], paragraphs)\n",
    "        # 考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        # 拼接tokens_a和tokens_b，添加特殊词元\n",
    "        # tokens:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "        # segments:[0, 0, 0, 0, 1, 1, 1, 1]\n",
    "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        # nsp_data_from_paragraph:List[Tuple[List[str], List[int], bool]]:每个元素是一个元组(tokens, segments, is_next)\n",
    "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86277b80",
   "metadata": {
    "origin_pos": 10
   },
   "source": [
    "### 生成遮蔽语言模型任务的数据\n",
    ":label:`subsec_prepare_mlm_data`\n",
    "\n",
    "为了从BERT输入序列生成遮蔽语言模型的训练样本，我们定义了以下`_replace_mlm_tokens`函数。在其输入中，`tokens`是表示BERT输入序列的词元的列表，`candidate_pred_positions`是不包括特殊词元的BERT输入序列的词元索引的列表（特殊词元在遮蔽语言模型任务中不被预测），以及`num_mlm_preds`指示预测的数量（选择15%要预测的随机词元）。在 :numref:`subsec_mlm`中定义遮蔽语言模型任务之后，在每个预测位置，输入可以由特殊的“掩码”词元或随机词元替换，或者保持不变。最后，该函数返回可能替换后的输入词元、发生预测的词元索引和这些预测的标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5e3de2c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.147428Z",
     "iopub.status.busy": "2023-08-18T07:00:41.146946Z",
     "iopub.status.idle": "2023-08-18T07:00:41.155481Z",
     "shell.execute_reply": "2023-08-18T07:00:41.154569Z"
    },
    "origin_pos": 11,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# tokens  --mask-->  mlm_input_tokens\n",
    "# 这段代码实现的是 BERT 模型中 Masked Language Model (MLM) 任务的核心逻辑\n",
    "# 对输入序列中的某些词元（tokens）进行遮蔽（masking）或替换，同时记录被遮蔽位置及其原始标签，用于后续的 MLM 训练任务。\n",
    "# tokens:List[str]:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "# candidate_pred_positions: 候选预测位置的索引列表(排除掉'<cls>'和'<sep>'的位置索引)\n",
    "# num_mlm_preds: 要预测的遮蔽词元数量 = tokens长度 * 0.15 四舍五入到最近的整数\n",
    "# vocab: 词汇表对象，用于将索引转换为词元\n",
    "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab):\n",
    "    # 深拷贝副本，为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    mlm_input_tokens = [token for token in tokens] # 深拷贝\n",
    "    # 记录被遮蔽位置及其原始标签\n",
    "    pred_positions_and_labels = [] \n",
    "    # 打乱可遮蔽位置(候选位置)的索引列表\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "    \n",
    "        # 这正是 BERT 论文中描述的 MLM 的 80-10-10 策略\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>' # 80%的时间：将词替换为“<mask>”词元\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position] # 10%的时间：保持词不变\n",
    "            else:\n",
    "                masked_token = random.choice(vocab.idx_to_token) # 10%的时间：用随机词替换该词\n",
    "\n",
    "        # 实施词元替换\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        # 记录被遮蔽位置及其原始标签(真实值)\n",
    "        pred_positions_and_labels.append((mlm_pred_position, tokens[mlm_pred_position]))\n",
    "\n",
    "    return mlm_input_tokens, pred_positions_and_labels\n",
    "    # mlm_input_tokens: 更新后的tokens,包含遮蔽语言模型任务输入的词元列表\n",
    "    # pred_positions_and_labels: 被遮蔽位置及其原始标签的元组列表"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ce2383",
   "metadata": {
    "origin_pos": 12
   },
   "source": [
    "通过调用前述的`_replace_mlm_tokens`函数，以下函数将BERT输入序列（`tokens`）作为输入，并返回输入词元的索引（在 :numref:`subsec_mlm`中描述的可能的词元替换之后）、发生预测的词元索引以及这些预测的标签索引。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "841a4650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.160061Z",
     "iopub.status.busy": "2023-08-18T07:00:41.159300Z",
     "iopub.status.idle": "2023-08-18T07:00:41.165820Z",
     "shell.execute_reply": "2023-08-18T07:00:41.164855Z"
    },
    "origin_pos": 13,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# tokens:List[str]:['<cls>', 'hello', 'world', '<sep>', 'how', 'are', 'you', '<sep>']\n",
    "# 这个函数只能处理单句的tokens, 不能处理批量的tokens\n",
    "def _get_mlm_data_from_tokens(tokens, vocab):\n",
    "    candidate_pred_positions = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        # 排除'<cls>'和'<sep>'的位置索引\n",
    "        candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    num_mlm_preds = max(1, round(len(tokens) * 0.15)) # round：四舍五入到最近的整数\n",
    "    # tokens  --mask-->  mlm_input_tokens\n",
    "    # pred_positions_and_labels: (position, label)\n",
    "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
    "    # 对于列表中的每一个元素 x，取它的第 0 个元素（即 x[0]）作为排序的“关键字”。\n",
    "    # 重新回到原始的tokens顺序\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels, key=lambda x: x[0])\n",
    "    # 位置索引\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    # 预测标签\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    # vacab[mlm_input_tokens]: 将mlm_input_tokens(添加mask后的tokens)中的每个词元转换为对应的索引\n",
    "    # vacab[mlm_pred_labels]: 将mlm_pred_labels(被遮蔽位置的原始标签)中的每个词元转换为对应的索引\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n",
    "    # (input_ids, pred_positions, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396550b1",
   "metadata": {
    "origin_pos": 14
   },
   "source": [
    "## 将文本转换为预训练数据集\n",
    "\n",
    "现在我们几乎准备好为BERT预训练定制一个`Dataset`类。在此之前，我们仍然需要定义辅助函数`_pad_bert_inputs`来将特殊的“&lt;mask&gt;”词元附加到输入。它的参数`examples`包含来自两个预训练任务的辅助函数`_get_nsp_data_from_paragraph`和`_get_mlm_data_from_tokens`的输出。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6552099b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.170203Z",
     "iopub.status.busy": "2023-08-18T07:00:41.169578Z",
     "iopub.status.idle": "2023-08-18T07:00:41.180126Z",
     "shell.execute_reply": "2023-08-18T07:00:41.179219Z"
    },
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# examples:(mlm_input_ids, pred_positions, mlm_pred_labels_ids, segments, is_next)\n",
    "def _pad_bert_inputs(examples, max_len, vocab):\n",
    "    max_num_mlm_preds = round(max_len * 0.15) # 四舍五入\n",
    "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels_ids = [], [], []\n",
    "    nsp_labels = []\n",
    "\n",
    "    for (mlm_input_ids, pred_positions, mlm_pred_label_ids, segments, is_next) in examples:\n",
    "        # 填充词元到最大长度\n",
    "        all_token_ids.append(torch.tensor(mlm_input_ids + [vocab['<pad>']] * (max_len - len(mlm_input_ids)), dtype=torch.long))\n",
    "        # 填充句子对的片段索引到最大长度\n",
    "        all_segments.append(torch.tensor(segments + [0] * (max_len - len(segments)), dtype=torch.long))\n",
    "        # valid_lens不包括'<pad>'的计数\n",
    "        valid_lens.append(torch.tensor(len(mlm_input_ids), dtype=torch.float32))\n",
    "        # 填充预测位置到最大预测数\n",
    "        all_pred_positions.append(torch.tensor(pred_positions + [0] * (max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # 填充词元的预测将通过乘以0权重在损失中过滤掉\n",
    "        all_mlm_weights.append(torch.tensor([1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.float32))\n",
    "        all_mlm_labels_ids.append(torch.tensor(mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids)), dtype=torch.long))\n",
    "        nsp_labels.append(torch.tensor(is_next, dtype=torch.long))\n",
    "    return (\n",
    "    all_token_ids,           # [batch_size, max_len]\n",
    "    all_segments,            # [batch_size, max_len]\n",
    "    valid_lens,              # [batch_size]\n",
    "    all_pred_positions,      # [batch_size, max_num_mlm_preds]\n",
    "    all_mlm_weights,         # [batch_size, max_num_mlm_preds]\n",
    "    all_mlm_labels_ids,      # [batch_size, max_num_mlm_preds]\n",
    "    nsp_labels               # [batch_size]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8a88c",
   "metadata": {
    "origin_pos": 18
   },
   "source": [
    "将用于生成两个预训练任务的训练样本的辅助函数和用于填充输入的辅助函数放在一起，我们定义以下`_WikiTextDataset`类为用于预训练BERT的WikiText-2数据集。通过实现`__getitem__ `函数，我们可以任意访问WikiText-2语料库的一对句子生成的预训练样本（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "最初的BERT模型使用词表大小为30000的WordPiece嵌入 :cite:`Wu.Schuster.Chen.ea.2016`。WordPiece的词元化方法是对 :numref:`subsec_Byte_Pair_Encoding`中原有的字节对编码算法稍作修改。为简单起见，我们使用`d2l.tokenize`函数进行词元化。出现次数少于5次的不频繁词元将被过滤掉。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d049c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.184551Z",
     "iopub.status.busy": "2023-08-18T07:00:41.183947Z",
     "iopub.status.idle": "2023-08-18T07:00:41.192539Z",
     "shell.execute_reply": "2023-08-18T07:00:41.191426Z"
    },
    "origin_pos": 20,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "# 把句子拆分成词元列表\n",
    "class _WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        # paragraphs[行，行中的句子]:List[List[Str]]\n",
    "        # paragraph[句子列表]:List[Str]\n",
    "        # d2l.tokenize:把句子拆分成词元列表:List[List[Str]]\n",
    "        # 左边paragraphs[词元列表]:List[List[List[Str]]]\n",
    "        # paragraphs: List[ List[ List[str] ] ]\n",
    "        #             ↑      ↑      ↑\n",
    "        #            段落   句子   词（token）\n",
    "        paragraphs = [d2l.tokenize(paragraph, token='word') for paragraph in paragraphs]\n",
    "        # [表达式 for 外层变量 in 外层可迭代对象 for 内层变量 in 内层可迭代对象]\n",
    "        # paragraphs:List[List[List[Str]]]\n",
    "        # paragraph:List[List[Str]]\n",
    "        # sentence:List[Str]\n",
    "        # sentences:List[List[Str]]:整个数据集的所有句子\n",
    "        sentences = [sentence for paragraph in paragraphs for sentence in paragraph]\n",
    "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=['<pad>', '<mask>', '<cls>', '<sep>'])\n",
    "        \n",
    "        examples = []\n",
    "        # 分解\n",
    "        # paragraphs:List[List[List[Str]]]\n",
    "        # paragraph:List[List[Str]]\n",
    "        # 获取下一句子预测任务NSP的数据\n",
    "        # _get_nsp_data_from_paragraph:List[Tuple[List[str], List[int], bool]]\n",
    "        # 分解\n",
    "        # example:(tokens, segments, is_next)\n",
    "        # 合并\n",
    "        # examples:List[Tuple[List[str], List[int], bool]]\n",
    "        # 此时examples列表中每个元素是一个元组(tokens, segments, is_next), 这个元组信息来源于之前的一个段落(1行)\n",
    "        examples = [example for paragraph in paragraphs for example in _get_nsp_data_from_paragraph(paragraph, paragraphs, self.vocab, max_len)]\n",
    "        for example in examples[:3]:\n",
    "            print(example)\n",
    "        # 获取遮蔽语言模型任务MLM的数据\n",
    "        # 这里用到元组拼接，结果：(mlm_input_ids, pred_positions, mlm_pred_labels_ids, segments, is_next)\n",
    "        # 在_get_mlm_data_from_tokens中，所有的词元都被转换为了词元ID\n",
    "        examples = [_get_mlm_data_from_tokens(tokens, self.vocab) + (segments, is_next) for tokens, segments, is_next in examples]\n",
    "        for example in examples[:3]:\n",
    "            print(example)\n",
    "        # 填充输入\n",
    "        # _pad_bert_inputs:填充输入，返回元组\n",
    "        # (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels_ids, self.nsp_labels)\n",
    "        (self.all_token_ids, self.all_segments, self.valid_lens, self.all_pred_positions, self.all_mlm_weights, self.all_mlm_labels_ids, self.nsp_labels) = _pad_bert_inputs(examples, max_len, self.vocab)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx],\n",
    "                self.all_segments[idx],\n",
    "                self.valid_lens[idx], \n",
    "                self.all_pred_positions[idx],\n",
    "                self.all_mlm_weights[idx], \n",
    "                self.all_mlm_labels_ids[idx],\n",
    "                self.nsp_labels[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede31c0",
   "metadata": {
    "origin_pos": 22
   },
   "source": [
    "通过使用`_read_wiki`函数和`_WikiTextDataset`类，我们定义了下面的`load_data_wiki`来下载并生成WikiText-2数据集，并从中生成预训练样本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b484a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.197261Z",
     "iopub.status.busy": "2023-08-18T07:00:41.196591Z",
     "iopub.status.idle": "2023-08-18T07:00:41.202074Z",
     "shell.execute_reply": "2023-08-18T07:00:41.201154Z"
    },
    "origin_pos": 24,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    \"\"\"加载WikiText-2数据集\"\"\"\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    # paragraphs:List[List[Str]]\n",
    "    paragraphs = _read_wiki()\n",
    "    # train_set: all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
    "    # train_iter: all_token_ids, all_segments, valid_lens, all_pred_positions, all_mlm_weights, all_mlm_labels, nsp_labels\n",
    "    train_iter = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True, num_workers=num_workers)\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b59eb9",
   "metadata": {
    "origin_pos": 26
   },
   "source": [
    "将批量大小设置为512，将BERT输入序列的最大长度设置为64，我们打印出小批量的BERT预训练样本的形状。注意，在每个BERT输入序列中，为遮蔽语言模型任务预测$10$（$64 \\times 0.15$）个位置。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f1a8e103",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:41.206083Z",
     "iopub.status.busy": "2023-08-18T07:00:41.205815Z",
     "iopub.status.idle": "2023-08-18T07:00:52.152614Z",
     "shell.execute_reply": "2023-08-18T07:00:52.151321Z"
    },
    "origin_pos": 27,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['<cls>', 'the', 'secwepemc', 'people', 'have', 'lived', 'in', 'the', 'adams', 'river', 'valley', 'for', 'millennia', '<sep>', 'a', '1977', 'study', 'by', 'the', 'provincial', 'government', 'along', 'the', 'lower', 'river', 'found', 'sixty', '@-@', 'six', 'sites', 'with', 'evidence', 'of', 'habitation', 'dating', 'to', '2000', 'bce', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n",
      "(['<cls>', 'the', 'soldiers', 'were', 'told', 'by', 'this', 'agent', 'that', '\"', 'the', 'orders', 'were', 'to', 'kill', 'them', 'all', '\"', ',', 'never', 'mind', 'that', 'only', 'civilians', ',', 'women', 'and', 'children', 'included', ',', 'were', 'found', '<sep>', 'the', 'anthem', 'was', 'first', 'performed', 'in', 'lithuania', 'during', 'the', 'great', '<unk>', 'of', '<unk>', 'on', 'december', '3', ',', '1905', '.', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "(['<cls>', 'all', 'of', 'the', 'victims', 'were', 'civilians', '<sep>', 'the', 'massacre', 'was', 'recounted', 'in', 'july', '1973', 'by', 'the', 'british', 'catholic', 'priest', ',', 'father', 'adrian', 'hastings', ',', 'and', 'two', 'other', 'spanish', 'missionary', 'priests', '<sep>'], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n",
      "([3, 5, 0, 154, 51, 1052, 9, 5, 3014, 2, 1084, 20, 17973, 4, 11, 3015, 629, 22, 5, 4321, 212, 164, 5, 736, 187, 150, 5367, 14, 213, 1592, 2, 686, 7, 14876, 2, 10, 977, 2, 4], [9, 21, 30, 34, 35, 37], [187, 164, 21, 2473, 10, 2973], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n",
      "([3, 5, 1060, 31, 2, 22, 37, 2456, 2, 12, 2, 1726, 31, 10, 2, 100, 66, 12, 6, 411, 2247, 17, 74, 4565, 6, 500, 8, 363, 241, 6, 31, 150, 4, 2, 3915, 13, 2, 470, 9, 6293, 2, 5, 325, 0, 7, 0, 15, 284, 97, 2, 3914, 18, 4], [4, 8, 10, 14, 33, 36, 40, 49], [820, 17, 5, 2073, 5, 40, 61, 6], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], False)\n",
      "([3, 66, 7, 5, 3093, 31, 4565, 4, 2, 7766, 13, 11326, 9, 232, 2373, 22, 5, 2, 1261, 3573, 6, 469, 2, 9313, 2, 8, 50, 62, 2, 7767, 5145, 4], [8, 17, 22, 24, 28], [5, 161, 7064, 6, 1227], [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], True)\n"
     ]
    }
   ],
   "source": [
    "# max_len 表示每个样本的最大长度\n",
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "34066341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64])\n",
      "torch.Size([512, 64])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512, 10])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X, mlm_Y, nsp_y) in train_iter:\n",
    "    print(f\"{tokens_X.shape}\\n\"\n",
    "          f\"{segments_X.shape}\\n\"\n",
    "          f\"{valid_lens_x.shape}\\n\"\n",
    "          f\"{pred_positions_X.shape}\\n\"\n",
    "          f\"{mlm_weights_X.shape}\\n\"\n",
    "          f\"{mlm_Y.shape}\\n\"\n",
    "          f\"{nsp_y.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b78dd7",
   "metadata": {
    "origin_pos": 28
   },
   "source": [
    "最后，我们来看一下词量。即使在过滤掉不频繁的词元之后，它仍然比PTB数据集的大两倍以上。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "47b86684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-18T07:00:52.159404Z",
     "iopub.status.busy": "2023-08-18T07:00:52.158958Z",
     "iopub.status.idle": "2023-08-18T07:00:52.169643Z",
     "shell.execute_reply": "2023-08-18T07:00:52.168438Z"
    },
    "origin_pos": 29,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081adbe2",
   "metadata": {
    "origin_pos": 30
   },
   "source": [
    "## 小结\n",
    "\n",
    "* 与PTB数据集相比，WikiText-2数据集保留了原来的标点符号、大小写和数字，并且比PTB数据集大了两倍多。\n",
    "* 我们可以任意访问从WikiText-2语料库中的一对句子生成的预训练（遮蔽语言模型和下一句预测）样本。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 为简单起见，句号用作拆分句子的唯一分隔符。尝试其他的句子拆分技术，比如Spacy和NLTK。以NLTK为例，需要先安装NLTK：`pip install nltk`。在代码中先`import nltk`。然后下载Punkt语句词元分析器：`nltk.download('punkt')`。要拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用`nltk.tokenize.sent_tokenize(sentences)`将返回两个句子字符串的列表：`['This is great !', 'Why not ?']`。\n",
    "1. 如果我们不过滤出一些不常见的词元，词量会有多大？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcf3ae",
   "metadata": {
    "origin_pos": 32,
    "tab": [
     "pytorch"
    ]
   },
   "source": [
    "[Discussions](https://discuss.d2l.ai/t/5738)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "required_libs": []
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
